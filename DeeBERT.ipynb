{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cgcez2ftFR3",
        "outputId": "c068a5ea-e9e2-4b74-fee0-2372baffd84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/NLP/DeeBERT\"\n",
        "os.chdir(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBlw3BzstgWA",
        "outputId": "e445a3a9-574a-4560-e73a-a948e77cdcd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 2)) (4.62.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (1.20.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 6)) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 10)) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 12)) (0.0.46)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.14 in /usr/local/lib/python3.7/dist-packages (from boto3->-r ./requirements.txt (line 4)) (1.23.14)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->-r ./requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->-r ./requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.14->boto3->-r ./requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.14->boto3->-r ./requirements.txt (line 4)) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.14->boto3->-r ./requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r ./requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r ./requirements.txt (line 6)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r ./requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r ./requirements.txt (line 12)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r ./requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r ./examples/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirements.txt\n",
        "!pip install -r ./examples/requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t8IsAAPadUR"
      },
      "source": [
        "### Download GLUE dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAKIu1S2vkUW",
        "outputId": "6799c519-1904-458c-90da-847b87414513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tCompleted!\n"
          ]
        }
      ],
      "source": [
        "!python download_glue_data.py --data_dir glue_data --tasks all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG7ZYWt4adUV"
      },
      "source": [
        "### Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "03UBcgP3adUW"
      },
      "outputs": [],
      "source": [
        "PATH_TO_DATA=\"glue_data\"\n",
        "MODEL_TYPE=\"bert\"  # bert or roberta\n",
        "MODEL_SIZE=\"base\"  # base or large\n",
        "DATASET=\"MRPC\"  # SST-2, MRPC, RTE, QNLI, QQP, or MNLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpEL7einadUX"
      },
      "source": [
        "### This is for fine-tuning and evaluating models as in the original BERT paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLCNKcVT02ys",
        "outputId": "4aa05255-9ea7-4c80-95c6-f7e207e3d734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-11-29 02:59:27.406445: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 02:59:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 02:59:27 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "11/29/2021 02:59:27 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 02:59:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/29/2021 02:59:27 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/29/2021 02:59:31 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "11/29/2021 02:59:31 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "11/29/2021 02:59:32 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/raw', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, save_steps=0, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 02:59:32 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   LOOKING AT glue_data/MRPC/train.tsv\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 02:59:32 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 02:59:35 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_train_bert-base-uncased_128_mrpc\n",
            "11/29/2021 02:59:36 - INFO - __main__ -   ***** Running training *****\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Num examples = 3668\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Num Epochs = 3\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "11/29/2021 02:59:36 - INFO - __main__ -     Total optimization steps = 1377\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A/content/drive/My Drive/NLP/DeeBERT/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/459 [00:00<04:43,  1.62it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:01<03:55,  1.94it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:40,  2.07it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:32,  2.14it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:28,  2.18it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:23,  2.22it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:23,  2.22it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:21,  2.23it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:20,  2.23it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:05<03:21,  2.22it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:22,  2.21it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:20,  2.22it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:19,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:18,  2.24it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:18,  2.23it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:17,  2.23it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:17,  2.24it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:16,  2.24it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:09<03:16,  2.24it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:16,  2.22it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:10<03:15,  2.23it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:15,  2.23it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:14,  2.23it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:14,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:14,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:13,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:12,  2.24it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:13<03:12,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:12,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:14<03:11,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:11,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:14<03:10,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:10,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:10,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:09,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:09,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:09,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:08,  2.23it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:18<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:18<03:07,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:06,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:19<03:06,  2.22it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:06,  2.22it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:20<03:05,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:05,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:04,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:22<03:03,  2.23it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  11% 50/459 [00:22<03:03,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:23<03:03,  2.22it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:03,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:23<03:02,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:02,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:24<03:02,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:01,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:25<03:01,  2.22it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:00,  2.22it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<02:59,  2.22it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:27<02:59,  2.22it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<03:00,  2.21it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:27<02:59,  2.21it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<02:58,  2.22it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:28<02:58,  2.22it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<02:58,  2.21it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:29<02:57,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:56,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:30<02:56,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:31<02:55,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:55,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:32<02:55,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:54,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:32<02:54,  2.22it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:53,  2.22it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:33<02:53,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:53,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:34<02:52,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:51,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:35<02:51,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:36<02:51,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:36<02:50,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:37<02:49,  2.22it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:49,  2.22it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:37<02:48,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:48,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:38<02:48,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:47,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:39<02:47,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:40<02:46,  2.22it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:40<02:46,  2.22it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:41<02:45,  2.22it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:41<02:44,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:41<02:44,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:43,  2.23it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:42<02:43,  2.22it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:43,  2.22it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:43<02:43,  2.22it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:44<02:42,  2.22it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:44<02:42,  2.22it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:45<02:41,  2.22it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:46<02:40,  2.22it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:46<02:40,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:46<02:39,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:39,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:47<02:38,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:38,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:48<02:37,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:49<02:37,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:49<02:37,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:50<02:36,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:50<02:36,  2.21it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:50<02:36,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:51<02:35,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:51<02:35,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:34,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:52<02:34,  2.22it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:53<02:33,  2.22it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:53<02:33,  2.22it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:54<02:32,  2.22it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:54<02:32,  2.22it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:55<02:31,  2.22it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:55<02:30,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:55<02:30,  2.22it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:56<02:30,  2.21it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:56<02:29,  2.22it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:57<02:29,  2.23it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:57<02:28,  2.23it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:58<02:27,  2.23it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:58<02:27,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:59<02:27,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:59<02:27,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [00:59<02:26,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:00<02:26,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:00<02:26,  2.21it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:01<02:25,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:01<02:25,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:02<02:24,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:02<02:24,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:03<02:23,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:03<02:23,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:04<02:22,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:04<02:22,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:04<02:21,  2.22it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:05<02:21,  2.22it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:05<02:20,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:06<02:19,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:06<02:19,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:07<02:19,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:07<02:18,  2.22it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:08<02:18,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:08<02:17,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:08<02:17,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:09<02:16,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:09<02:16,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:10<02:17,  2.21it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:10<02:15,  2.22it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:11<02:15,  2.23it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:11<02:14,  2.22it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:12<02:14,  2.22it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:12<02:13,  2.23it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:13<02:13,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:13<02:12,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:13<02:12,  2.22it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:14<02:11,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:14<02:11,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:15<02:11,  2.22it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:15<02:10,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:16<02:10,  2.22it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:16<02:10,  2.22it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:17<02:09,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:17<02:08,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:17<02:08,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:18<02:08,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:18<02:07,  2.22it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:19<02:06,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:19<02:07,  2.22it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:20<02:06,  2.22it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:20<02:06,  2.22it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:21<02:05,  2.22it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:21<02:05,  2.22it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:22<02:04,  2.23it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:22<02:04,  2.22it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:22<02:03,  2.23it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:23<02:03,  2.22it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:23<02:02,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:24<02:02,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:24<02:01,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:25<02:01,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:25<02:01,  2.22it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:26<02:00,  2.22it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:26<01:59,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:26<01:59,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:27<01:58,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:27<01:58,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:28<01:57,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:28<01:57,  2.24it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:29<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:29<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:30<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:30<01:55,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:30<01:55,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:31<01:54,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:31<01:54,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:32<01:53,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:32<01:53,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:33<01:52,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:33<01:52,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:34<01:52,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:34<01:51,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:35<01:51,  2.22it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:35<01:51,  2.22it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:35<01:50,  2.22it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:36<01:50,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:36<01:49,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:37<01:49,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:37<01:48,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:38<01:48,  2.23it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:38<01:47,  2.22it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:39<01:47,  2.22it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:39<01:47,  2.22it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:39<01:46,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:40<01:46,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:40<01:45,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:41<01:44,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:41<01:44,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:42<01:44,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:42<01:43,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:43<01:43,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:43<01:42,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:43<01:42,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:44<01:41,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:44<01:41,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:45<01:41,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:45<01:40,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:46<01:40,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:46<01:39,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:47<01:39,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:47<01:38,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:48<01:38,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:48<01:37,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:48<01:37,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:49<01:36,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:49<01:36,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:50<01:36,  2.23it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:50<01:35,  2.23it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:51<01:35,  2.23it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:51<01:34,  2.24it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:52<01:33,  2.23it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:52<01:33,  2.24it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:52<01:32,  2.24it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:53<01:32,  2.23it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:53<01:32,  2.23it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:54<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:54<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:55<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:55<01:30,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:56<01:29,  2.24it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:56<01:29,  2.24it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:56<01:28,  2.24it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:57<01:28,  2.24it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:57<01:28,  2.23it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:58<01:27,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:58<01:27,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [01:59<01:27,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [01:59<01:26,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [02:00<01:26,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:00<01:25,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:01<01:25,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:01<01:24,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:01<01:24,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:02<01:23,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:02<01:23,  2.23it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:03<01:22,  2.23it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:03<01:22,  2.24it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:04<01:21,  2.23it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:04<01:21,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:05<01:21,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:05<01:20,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:05<01:20,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:06<01:19,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:06<01:19,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:07<01:18,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:07<01:18,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:08<01:18,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:08<01:17,  2.22it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:09<01:17,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:09<01:16,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:10<01:16,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:10<01:16,  2.22it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:10<01:15,  2.22it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:11<01:15,  2.22it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:11<01:14,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:12<01:13,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:12<01:13,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:13<01:13,  2.22it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:13<01:12,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:14<01:12,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:14<01:11,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:14<01:11,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:15<01:11,  2.22it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:15<01:10,  2.22it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:16<01:10,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:16<01:09,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:17<01:09,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:17<01:08,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:18<01:08,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:18<01:07,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:18<01:07,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:19<01:07,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:19<01:06,  2.21it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:20<01:06,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:20<01:05,  2.21it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:21<01:05,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:21<01:05,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:22<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:22<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:23<01:03,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:23<01:03,  2.22it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:23<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:24<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:24<01:02,  2.20it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:25<01:01,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:25<01:01,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:26<01:00,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:26<01:00,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:27<00:59,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:27<00:59,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:28<00:58,  2.22it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:28<00:58,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:28<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:29<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:29<00:56,  2.22it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:30<00:56,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:30<00:56,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:31<00:55,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:31<00:55,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:32<00:54,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:32<00:54,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:33<00:53,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:33<00:53,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:33<00:52,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:34<00:52,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:34<00:51,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:35<00:51,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:35<00:51,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:36<00:50,  2.20it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:36<00:50,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:37<00:49,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:37<00:49,  2.20it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:38<00:48,  2.21it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:38<00:48,  2.20it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:38<00:47,  2.21it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:39<00:47,  2.21it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:39<00:46,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:40<00:46,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:40<00:46,  2.21it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:41<00:45,  2.21it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:41<00:45,  2.21it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:42<00:44,  2.21it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:42<00:44,  2.20it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:42<00:44,  2.20it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:43<00:43,  2.20it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:43<00:43,  2.20it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:44<00:42,  2.21it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:44<00:42,  2.21it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:45<00:41,  2.21it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:45<00:41,  2.21it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:46<00:40,  2.21it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:46<00:40,  2.21it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:47<00:39,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:47<00:39,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:47<00:39,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:48<00:38,  2.21it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:48<00:37,  2.22it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:49<00:37,  2.22it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:49<00:36,  2.22it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:50<00:36,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:50<00:36,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:51<00:35,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:51<00:35,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:52<00:34,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:52<00:34,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:52<00:34,  2.20it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:53<00:33,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:53<00:33,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:54<00:32,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:54<00:32,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:55<00:31,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:55<00:31,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:56<00:30,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:56<00:30,  2.21it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:57<00:29,  2.21it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:57<00:29,  2.21it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:57<00:28,  2.21it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:58<00:28,  2.22it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:58<00:27,  2.22it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:59<00:27,  2.22it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [02:59<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [03:00<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [03:00<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:01<00:25,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:01<00:25,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:01<00:24,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:02<00:24,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:02<00:23,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:03<00:23,  2.21it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:03<00:23,  2.21it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:04<00:22,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:04<00:22,  2.21it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:05<00:21,  2.21it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:05<00:21,  2.21it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:06<00:20,  2.20it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:06<00:20,  2.20it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:06<00:19,  2.20it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:07<00:19,  2.20it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:07<00:19,  2.20it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:08<00:18,  2.20it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:08<00:18,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:09<00:17,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:09<00:17,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:10<00:16,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:10<00:16,  2.21it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:11<00:15,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:11<00:15,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:11<00:14,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:12<00:14,  2.21it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:12<00:14,  2.21it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:13<00:13,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:13<00:13,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:14<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:14<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:15<00:11,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:15<00:11,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:16<00:10,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:16<00:10,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:16<00:09,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:17<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:17<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:18<00:08,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:18<00:08,  2.20it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:19<00:07,  2.20it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:19<00:07,  2.21it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:20<00:06,  2.20it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:20<00:06,  2.21it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:21<00:05,  2.21it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:21<00:05,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:21<00:04,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:22<00:04,  2.22it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:22<00:04,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:23<00:03,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:23<00:03,  2.22it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:24<00:02,  2.22it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:24<00:02,  2.21it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:25<00:01,  2.20it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:25<00:01,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:25<00:00,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:26<00:00,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:26<00:00,  2.22it/s]\n",
            "Epoch:  33% 1/3 [03:26<06:53, 206.71s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:27,  2.21it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:27,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:27,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:27,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:26,  2.19it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:24,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:24,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:23,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:04<03:22,  2.21it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:22,  2.21it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:21,  2.21it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:20,  2.22it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:20,  2.21it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:20,  2.21it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:19,  2.21it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:19,  2.21it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:19,  2.21it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:09<03:18,  2.21it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:17,  2.21it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:09<03:17,  2.22it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:16,  2.22it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:17,  2.21it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:16,  2.20it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:15,  2.21it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:15,  2.21it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:14,  2.21it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:13<03:14,  2.21it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:14,  2.20it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:14<03:13,  2.21it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:13,  2.21it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:14<03:13,  2.20it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:12,  2.21it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:11,  2.21it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:10,  2.21it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:10,  2.22it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:09,  2.22it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:18<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:18<03:07,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:07,  2.22it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:19<03:07,  2.21it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:07,  2.21it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:20<03:06,  2.21it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:06,  2.21it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:05,  2.21it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:22<03:04,  2.22it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:22<03:04,  2.22it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:23<03:04,  2.21it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:04,  2.21it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:23<03:03,  2.21it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:03,  2.21it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:24<03:02,  2.21it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:02,  2.21it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:25<03:02,  2.21it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:01,  2.21it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<03:01,  2.21it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:27<03:01,  2.20it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<03:00,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:28<03:00,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<02:59,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:28<02:59,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<02:58,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:29<02:58,  2.20it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:57,  2.21it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:30<02:56,  2.21it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:31<02:56,  2.21it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:56,  2.21it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:32<02:56,  2.20it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:56,  2.20it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:33<02:55,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:54,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:33<02:54,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:53,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:34<02:53,  2.20it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:53,  2.20it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:35<02:53,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:36<02:52,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:36<02:52,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:37<02:51,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:50,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:38<02:50,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:49,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:38<02:48,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:48,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:39<02:47,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:40<02:47,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:40<02:47,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:41<02:46,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:41<02:46,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:42<02:46,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:45,  2.20it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:43<02:45,  2.20it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:44,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:43<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:44<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:44<02:43,  2.20it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:46<02:41,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:46<02:41,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:47<02:40,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:40,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:48<02:39,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:38,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:48<02:38,  2.21it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:49<02:38,  2.21it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:49<02:37,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:50<02:37,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:50<02:37,  2.21it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:51<02:37,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:51<02:36,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:52<02:36,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:35,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:52<02:35,  2.20it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:53<02:34,  2.21it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:53<02:34,  2.20it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:54<02:33,  2.21it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:54<02:33,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:55<02:32,  2.21it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:55<02:31,  2.21it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:56<02:31,  2.21it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:56<02:31,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:57<02:31,  2.20it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:57<02:30,  2.20it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:57<02:30,  2.20it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:58<02:29,  2.20it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:58<02:29,  2.21it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:59<02:28,  2.21it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:59<02:27,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [01:00<02:27,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:00<02:26,  2.22it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:01<02:26,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:01<02:25,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:02<02:25,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:02<02:25,  2.21it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:02<02:24,  2.21it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:03<02:24,  2.21it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:03<02:24,  2.21it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:04<02:23,  2.21it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:04<02:23,  2.21it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:05<02:22,  2.21it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:05<02:22,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:06<02:22,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:06<02:21,  2.21it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:07<02:21,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:07<02:20,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:07<02:20,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:08<02:20,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:08<02:19,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:09<02:19,  2.20it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:09<02:18,  2.21it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:10<02:17,  2.21it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:10<02:17,  2.20it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:11<02:16,  2.21it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:11<02:16,  2.21it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:12<02:15,  2.21it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:12<02:14,  2.22it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:12<02:14,  2.21it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:13<02:14,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:13<02:13,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:14<02:13,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:14<02:13,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:15<02:12,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:15<02:12,  2.21it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:16<02:11,  2.21it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:16<02:11,  2.21it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:16<02:10,  2.21it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:17<02:10,  2.21it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:17<02:10,  2.21it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:18<02:09,  2.21it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:18<02:09,  2.20it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:19<02:09,  2.20it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:19<02:08,  2.20it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:20<02:08,  2.20it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:20<02:07,  2.21it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:21<02:06,  2.21it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:21<02:07,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:21<02:06,  2.20it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:22<02:06,  2.20it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:22<02:05,  2.20it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:23<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:23<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:24<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:24<02:03,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:25<02:03,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:25<02:02,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:26<02:02,  2.20it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:26<02:01,  2.20it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:26<02:01,  2.20it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:27<02:00,  2.20it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:27<02:00,  2.21it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:28<01:59,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:28<01:59,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:29<01:58,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:29<01:58,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:30<01:58,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:30<01:57,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:31<01:57,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:31<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:31<01:56,  2.21it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:32<01:55,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:32<01:54,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:33<01:54,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:33<01:53,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:34<01:53,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:34<01:53,  2.20it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:35<01:52,  2.20it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:35<01:52,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:36<01:51,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:36<01:51,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:36<01:51,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:37<01:50,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:37<01:50,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:38<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:38<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:39<01:48,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:39<01:48,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:40<01:47,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:40<01:47,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:41<01:46,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:41<01:46,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:41<01:45,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:42<01:45,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:42<01:44,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:43<01:43,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:43<01:43,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:44<01:43,  2.21it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:44<01:42,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:45<01:42,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:45<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:45<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:46<01:40,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:46<01:40,  2.21it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:47<01:40,  2.21it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:47<01:40,  2.21it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:48<01:40,  2.20it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:48<01:39,  2.20it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:49<01:38,  2.21it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:49<01:38,  2.21it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:50<01:37,  2.21it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:50<01:37,  2.22it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:50<01:36,  2.21it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:51<01:36,  2.20it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:51<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:52<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:52<01:34,  2.21it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:53<01:34,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:53<01:34,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:54<01:33,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:54<01:33,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:55<01:32,  2.22it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:55<01:32,  2.22it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:55<01:31,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:56<01:31,  2.22it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:56<01:30,  2.22it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:57<01:30,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:57<01:30,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:58<01:29,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:58<01:29,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:59<01:28,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:59<01:28,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [02:00<01:27,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [02:00<01:27,  2.20it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [02:00<01:27,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:01<01:26,  2.21it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:01<01:26,  2.21it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:02<01:25,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:02<01:25,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:03<01:24,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:03<01:24,  2.20it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:04<01:24,  2.20it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:04<01:23,  2.20it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:05<01:22,  2.21it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:05<01:22,  2.21it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:05<01:21,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:06<01:21,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:06<01:20,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:07<01:20,  2.21it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:07<01:20,  2.20it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:08<01:20,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:08<01:19,  2.20it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:09<01:18,  2.20it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:09<01:18,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:10<01:18,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:10<01:17,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:10<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:11<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:11<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:12<01:15,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:12<01:15,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:13<01:14,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:13<01:14,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:14<01:13,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:14<01:13,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:14<01:12,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:15<01:12,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:15<01:11,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:16<01:11,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:16<01:11,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:17<01:10,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:17<01:10,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:18<01:09,  2.21it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:18<01:09,  2.21it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:19<01:09,  2.20it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:19<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:19<01:08,  2.20it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:20<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:20<01:07,  2.20it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:21<01:06,  2.20it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:21<01:06,  2.20it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:22<01:05,  2.20it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:22<01:05,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:23<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:23<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:24<01:03,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:24<01:03,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:24<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:25<01:01,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:25<01:01,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:26<01:00,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:26<01:00,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:27<01:00,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:27<00:59,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:28<00:59,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:28<00:58,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:28<00:58,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:29<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:29<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:30<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:30<00:56,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:31<00:56,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:31<00:55,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:32<00:55,  2.24it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:32<00:54,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:33<00:54,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:33<00:53,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:33<00:53,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:34<00:53,  2.23it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:34<00:52,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:35<00:52,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:35<00:51,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:36<00:51,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:36<00:50,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:37<00:50,  2.22it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:37<00:49,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:37<00:49,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:38<00:48,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:38<00:48,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:39<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:39<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:40<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:40<00:46,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:41<00:46,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:41<00:45,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:42<00:45,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:42<00:44,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:42<00:44,  2.21it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:43<00:44,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:43<00:43,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:44<00:43,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:44<00:42,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:45<00:42,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:45<00:41,  2.23it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:46<00:41,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:46<00:40,  2.23it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:46<00:40,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:47<00:39,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:47<00:39,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:48<00:38,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:48<00:38,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:49<00:38,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:49<00:37,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:50<00:37,  2.22it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:50<00:36,  2.22it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:50<00:36,  2.22it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:51<00:36,  2.22it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:51<00:35,  2.22it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:52<00:35,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:52<00:34,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:53<00:34,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:53<00:33,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:54<00:33,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:54<00:32,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:55<00:32,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:55<00:31,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:55<00:31,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:56<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:56<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:57<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:57<00:29,  2.24it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:58<00:29,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:58<00:28,  2.24it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:59<00:28,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:59<00:27,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:59<00:27,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [03:00<00:26,  2.24it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [03:00<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [03:01<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:01<00:25,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:02<00:25,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:02<00:24,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:03<00:24,  2.24it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:03<00:23,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:03<00:23,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:04<00:22,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:04<00:22,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:05<00:21,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:05<00:21,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:06<00:21,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:06<00:20,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:07<00:20,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:07<00:19,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:08<00:19,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:08<00:18,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:08<00:18,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:09<00:17,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:09<00:17,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:10<00:17,  2.21it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:10<00:16,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:11<00:16,  2.23it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:11<00:15,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:12<00:15,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:12<00:14,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:12<00:14,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:13<00:13,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:13<00:13,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:14<00:13,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:14<00:12,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:15<00:12,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:15<00:11,  2.24it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:16<00:11,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:16<00:10,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:17<00:10,  2.24it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:17<00:09,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:17<00:09,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:18<00:08,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:18<00:08,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:19<00:08,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:19<00:07,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:20<00:07,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:20<00:06,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:21<00:06,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:21<00:05,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:21<00:05,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:22<00:04,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:22<00:04,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:23<00:04,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:23<00:03,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:24<00:03,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:24<00:02,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:25<00:02,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:25<00:01,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:25<00:01,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:26<00:00,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:26<00:00,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:27<00:00,  2.22it/s]\n",
            "Epoch:  67% 2/3 [06:53<03:26, 206.97s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:25,  2.23it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:25,  2.22it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:24,  2.23it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:23,  2.23it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:23,  2.23it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:23,  2.23it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:22,  2.23it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:21,  2.24it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:20,  2.24it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:20,  2.24it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:04<03:20,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:20,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:19,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:19,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:19,  2.23it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:18,  2.23it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:18,  2.23it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:17,  2.23it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:16,  2.24it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:08<03:16,  2.23it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:16,  2.23it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:09<03:15,  2.24it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:15,  2.22it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:15,  2.22it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:15,  2.22it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:14,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:13,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:13,  2.23it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:12<03:12,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:12,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:13<03:12,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:11,  2.22it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:14<03:11,  2.23it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:11,  2.22it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:10,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:10,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:09,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:09,  2.23it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:08,  2.23it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:17<03:08,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:07,  2.23it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:18<03:07,  2.22it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:06,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:19<03:06,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:05,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:20<03:05,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:05,  2.23it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:04,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:21<03:03,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:22<03:03,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:22<03:02,  2.23it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:02,  2.22it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:23<03:02,  2.23it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:01,  2.23it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:24<03:01,  2.23it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:00,  2.23it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:25<03:00,  2.23it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:00,  2.22it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<02:59,  2.23it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:26<02:58,  2.23it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<02:58,  2.23it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:27<02:58,  2.23it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<02:57,  2.23it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:28<02:57,  2.23it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<02:57,  2.23it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:29<02:56,  2.23it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:55,  2.23it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:30<02:55,  2.22it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:30<02:55,  2.23it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:54,  2.23it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:31<02:54,  2.23it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:53,  2.23it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:32<02:53,  2.23it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:53,  2.22it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:33<02:52,  2.23it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:52,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:34<02:52,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:51,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:35<02:51,  2.22it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:35<02:50,  2.22it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:36<02:49,  2.23it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:36<02:49,  2.23it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:48,  2.23it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:37<02:48,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:48,  2.23it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:38<02:47,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:47,  2.23it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:39<02:46,  2.23it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:39<02:46,  2.22it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:40<02:45,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:40<02:45,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:41<02:44,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:41<02:44,  2.23it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:43,  2.24it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:42<02:42,  2.23it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:42,  2.23it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:43<02:42,  2.23it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:43<02:41,  2.23it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:44<02:41,  2.23it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:44<02:41,  2.23it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:45<02:40,  2.23it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:45<02:40,  2.23it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:46<02:39,  2.23it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:46<02:40,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:39,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:47<02:38,  2.23it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:37,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:48<02:37,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:48<02:37,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:49<02:36,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:49<02:36,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:50<02:35,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:50<02:35,  2.23it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:51<02:35,  2.22it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:51<02:34,  2.23it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:33,  2.23it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:52<02:33,  2.23it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:52<02:32,  2.23it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:53<02:32,  2.22it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:53<02:32,  2.23it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:54<02:31,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:54<02:31,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:55<02:30,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:55<02:30,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:56<02:29,  2.23it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:56<02:29,  2.23it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:57<02:29,  2.22it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:57<02:29,  2.22it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:57<02:28,  2.22it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:58<02:27,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:58<02:27,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:59<02:26,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [00:59<02:26,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:00<02:25,  2.23it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:00<02:25,  2.23it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:01<02:25,  2.23it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:01<02:24,  2.22it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:01<02:23,  2.23it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:02<02:23,  2.23it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:02<02:22,  2.23it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:03<02:22,  2.23it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:03<02:22,  2.22it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:04<02:22,  2.23it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:04<02:21,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:05<02:20,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:05<02:20,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:05<02:20,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:06<02:19,  2.23it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:06<02:18,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:07<02:18,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:07<02:17,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:08<02:17,  2.23it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:08<02:16,  2.24it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:09<02:16,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:09<02:16,  2.22it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:10<02:16,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:10<02:15,  2.23it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:10<02:14,  2.23it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:11<02:14,  2.24it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:11<02:14,  2.22it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:12<02:14,  2.22it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:12<02:13,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:13<02:12,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:13<02:12,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:14<02:11,  2.24it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:14<02:11,  2.23it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:14<02:11,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:15<02:10,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:15<02:10,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:16<02:09,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:16<02:09,  2.23it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:17<02:08,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:17<02:08,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:18<02:07,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:18<02:07,  2.23it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:18<02:07,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:19<02:06,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:19<02:05,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:20<02:05,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:20<02:05,  2.23it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:21<02:04,  2.24it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:21<02:03,  2.24it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:22<02:03,  2.24it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:22<02:02,  2.24it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:23<02:02,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:23<02:02,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:23<02:01,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:24<02:01,  2.23it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:24<02:01,  2.22it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:25<02:00,  2.22it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:25<02:00,  2.22it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:26<01:59,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:26<01:59,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:27<01:58,  2.23it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:27<01:58,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:27<01:58,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:28<01:57,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:28<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:29<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:29<01:56,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:30<01:55,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:30<01:55,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:31<01:54,  2.23it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:31<01:54,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:31<01:53,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:32<01:53,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:32<01:52,  2.23it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:33<01:52,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:33<01:51,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:34<01:51,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:34<01:51,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:35<01:50,  2.23it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:35<01:50,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:36<01:49,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:36<01:49,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:36<01:48,  2.23it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:37<01:48,  2.24it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:37<01:47,  2.23it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:38<01:47,  2.23it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:38<01:47,  2.23it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:39<01:46,  2.23it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:39<01:46,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:40<01:46,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:40<01:45,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:40<01:45,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:41<01:44,  2.23it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:41<01:43,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:42<01:43,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:42<01:43,  2.23it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:43<01:43,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:43<01:42,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:44<01:42,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:44<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:45<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:45<01:40,  2.23it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:45<01:40,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:46<01:39,  2.23it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:46<01:39,  2.22it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:47<01:38,  2.24it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:47<01:38,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:48<01:37,  2.24it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:48<01:37,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:49<01:36,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:49<01:36,  2.23it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:49<01:35,  2.24it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:50<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:50<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:51<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:51<01:34,  2.23it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:52<01:33,  2.23it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:52<01:32,  2.24it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:53<01:32,  2.23it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:53<01:32,  2.23it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:53<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:54<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:54<01:31,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:55<01:30,  2.22it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:55<01:29,  2.23it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:56<01:29,  2.23it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:56<01:29,  2.24it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:57<01:28,  2.23it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:57<01:28,  2.23it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:58<01:27,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:58<01:27,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [01:58<01:26,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [01:59<01:26,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [01:59<01:26,  2.23it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:00<01:25,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:00<01:25,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:01<01:24,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:01<01:24,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:02<01:24,  2.23it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:02<01:23,  2.23it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:02<01:22,  2.23it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:03<01:22,  2.24it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:03<01:21,  2.24it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:04<01:21,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:04<01:20,  2.24it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:05<01:20,  2.24it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:05<01:20,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:06<01:19,  2.23it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:06<01:19,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:06<01:18,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:07<01:18,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:07<01:17,  2.23it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:08<01:17,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:08<01:17,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:09<01:16,  2.23it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:09<01:16,  2.22it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:10<01:15,  2.22it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:10<01:15,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:11<01:14,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:11<01:14,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:11<01:13,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:12<01:13,  2.23it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:12<01:13,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:13<01:12,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:13<01:12,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:14<01:11,  2.23it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:14<01:11,  2.22it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:15<01:10,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:15<01:10,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:15<01:10,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:16<01:09,  2.23it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:16<01:09,  2.22it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:17<01:08,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:17<01:08,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:18<01:07,  2.23it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:18<01:07,  2.23it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:19<01:06,  2.23it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:19<01:06,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:19<01:05,  2.23it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:20<01:05,  2.23it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:20<01:05,  2.23it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:21<01:04,  2.23it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:21<01:03,  2.24it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:22<01:03,  2.23it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:22<01:03,  2.23it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:23<01:02,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:23<01:02,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:24<01:01,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:24<01:01,  2.23it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:24<01:00,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:25<01:00,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:25<00:59,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:26<00:59,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:26<00:59,  2.23it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:27<00:58,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:27<00:58,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:28<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:28<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:28<00:57,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:29<00:56,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:29<00:56,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:30<00:55,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:30<00:55,  2.23it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:31<00:54,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:31<00:54,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:32<00:53,  2.22it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:32<00:53,  2.23it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:32<00:52,  2.23it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:33<00:52,  2.23it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:33<00:52,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:34<00:51,  2.23it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:34<00:51,  2.22it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:35<00:50,  2.22it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:35<00:50,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:36<00:49,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:36<00:49,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:37<00:48,  2.23it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:37<00:48,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:37<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:38<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:38<00:47,  2.23it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:39<00:46,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:39<00:46,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:40<00:45,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:40<00:45,  2.23it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:41<00:44,  2.24it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:41<00:44,  2.23it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:41<00:43,  2.23it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:42<00:43,  2.23it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:42<00:43,  2.23it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:43<00:42,  2.23it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:43<00:42,  2.23it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:44<00:41,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:44<00:41,  2.23it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:45<00:40,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:45<00:40,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:45<00:39,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:46<00:39,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:46<00:39,  2.23it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:47<00:38,  2.22it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:47<00:38,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:48<00:37,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:48<00:37,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:49<00:36,  2.23it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:49<00:36,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:50<00:35,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:50<00:35,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:50<00:35,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:51<00:34,  2.23it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:51<00:34,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:52<00:33,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:52<00:33,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:53<00:32,  2.23it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:53<00:32,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:54<00:31,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:54<00:31,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:54<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:55<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:55<00:30,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:56<00:29,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:56<00:29,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:57<00:28,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:57<00:28,  2.23it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:58<00:27,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:58<00:27,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [02:59<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [02:59<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [02:59<00:26,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:00<00:25,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:00<00:25,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:01<00:24,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:01<00:24,  2.23it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:02<00:23,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:02<00:23,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:03<00:22,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:03<00:22,  2.23it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:03<00:22,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:04<00:21,  2.22it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:04<00:21,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:05<00:20,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:05<00:20,  2.23it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:06<00:19,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:06<00:19,  2.22it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:07<00:18,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:07<00:18,  2.23it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:07<00:17,  2.23it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:08<00:17,  2.23it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:08<00:17,  2.23it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:09<00:16,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:09<00:16,  2.23it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:10<00:15,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:10<00:15,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:11<00:14,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:11<00:14,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:12<00:13,  2.23it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:12<00:13,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:12<00:12,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:13<00:12,  2.23it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:13<00:12,  2.24it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:14<00:11,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:14<00:11,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:15<00:10,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:15<00:10,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:16<00:09,  2.23it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:16<00:09,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:16<00:08,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:17<00:08,  2.22it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:17<00:08,  2.23it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:18<00:07,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:18<00:07,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:19<00:06,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:19<00:06,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:20<00:05,  2.23it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:20<00:05,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:20<00:04,  2.24it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:21<00:04,  2.24it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:21<00:04,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:22<00:03,  2.23it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:22<00:03,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:23<00:02,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:23<00:02,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:24<00:01,  2.23it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:24<00:01,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:25<00:00,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:25<00:00,  2.23it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:25<00:00,  2.23it/s]\n",
            "Epoch: 100% 3/3 [10:19<00:00, 206.53s/it]\n",
            "11/29/2021 03:09:56 - INFO - __main__ -    global_step = 1377, average loss = 0.342229177986969\n",
            "11/29/2021 03:09:56 - INFO - __main__ -   Saving model checkpoint to ./saved_models/bert-base/MRPC/raw\n",
            "11/29/2021 03:09:56 - INFO - transformers.configuration_utils -   Configuration saved in ./saved_models/bert-base/MRPC/raw/config.json\n",
            "11/29/2021 03:10:02 - INFO - transformers.modeling_utils -   Model weights saved in ./saved_models/bert-base/MRPC/raw/pytorch_model.bin\n",
            "11/29/2021 03:10:03 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/raw/config.json\n",
            "11/29/2021 03:10:03 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:10:03 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/raw/pytorch_model.bin\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/raw' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/vocab.txt\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/added_tokens.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/special_tokens_map.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/tokenizer_config.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/raw' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/vocab.txt\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/added_tokens.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/special_tokens_map.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/raw/tokenizer_config.json\n",
            "11/29/2021 03:10:06 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/raw']\n",
            "11/29/2021 03:10:06 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/raw/config.json\n",
            "11/29/2021 03:10:06 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:10:06 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/raw/pytorch_model.bin\n",
            "11/29/2021 03:10:11 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:11 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:10:12 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc\n",
            "11/29/2021 03:10:12 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:10:12 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:10:12 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:11<00:00, 34.22it/s]\n",
            "11/29/2021 03:10:24 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:10:24 - INFO - __main__ -     acc = 0.8848039215686274\n",
            "11/29/2021 03:10:24 - INFO - __main__ -     acc_and_f1 = 0.9019544564813877\n",
            "11/29/2021 03:10:24 - INFO - __main__ -     f1 = 0.919104991394148\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/train.sh {PATH_TO_DATA} {MODEL_TYPE} {MODEL_SIZE} {DATASET}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vONhZ5TNadUa"
      },
      "source": [
        "### This is for fine-tuning DeeBERT models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJw_IZEW-6OB",
        "outputId": "da9973a1-b44a-46b4-c79f-6ddf0cfcb17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-11-29 03:10:33.551077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:10:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:10:34 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "11/29/2021 03:10:34 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:10:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/29/2021 03:10:34 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "11/29/2021 03:10:38 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['bert.encoder.highway.0.pooler.dense.weight', 'bert.encoder.highway.0.pooler.dense.bias', 'bert.encoder.highway.0.classifier.weight', 'bert.encoder.highway.0.classifier.bias', 'bert.encoder.highway.1.pooler.dense.weight', 'bert.encoder.highway.1.pooler.dense.bias', 'bert.encoder.highway.1.classifier.weight', 'bert.encoder.highway.1.classifier.bias', 'bert.encoder.highway.2.pooler.dense.weight', 'bert.encoder.highway.2.pooler.dense.bias', 'bert.encoder.highway.2.classifier.weight', 'bert.encoder.highway.2.classifier.bias', 'bert.encoder.highway.3.pooler.dense.weight', 'bert.encoder.highway.3.pooler.dense.bias', 'bert.encoder.highway.3.classifier.weight', 'bert.encoder.highway.3.classifier.bias', 'bert.encoder.highway.4.pooler.dense.weight', 'bert.encoder.highway.4.pooler.dense.bias', 'bert.encoder.highway.4.classifier.weight', 'bert.encoder.highway.4.classifier.bias', 'bert.encoder.highway.5.pooler.dense.weight', 'bert.encoder.highway.5.pooler.dense.bias', 'bert.encoder.highway.5.classifier.weight', 'bert.encoder.highway.5.classifier.bias', 'bert.encoder.highway.6.pooler.dense.weight', 'bert.encoder.highway.6.pooler.dense.bias', 'bert.encoder.highway.6.classifier.weight', 'bert.encoder.highway.6.classifier.bias', 'bert.encoder.highway.7.pooler.dense.weight', 'bert.encoder.highway.7.pooler.dense.bias', 'bert.encoder.highway.7.classifier.weight', 'bert.encoder.highway.7.classifier.bias', 'bert.encoder.highway.8.pooler.dense.weight', 'bert.encoder.highway.8.pooler.dense.bias', 'bert.encoder.highway.8.classifier.weight', 'bert.encoder.highway.8.classifier.bias', 'bert.encoder.highway.9.pooler.dense.weight', 'bert.encoder.highway.9.pooler.dense.bias', 'bert.encoder.highway.9.classifier.weight', 'bert.encoder.highway.9.classifier.bias', 'bert.encoder.highway.10.pooler.dense.weight', 'bert.encoder.highway.10.pooler.dense.bias', 'bert.encoder.highway.10.classifier.weight', 'bert.encoder.highway.10.classifier.bias', 'bert.encoder.highway.11.pooler.dense.weight', 'bert.encoder.highway.11.pooler.dense.bias', 'bert.encoder.highway.11.classifier.weight', 'bert.encoder.highway.11.classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "11/29/2021 03:10:38 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "-1\n",
            "11/29/2021 03:10:38 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, early_exit_entropy=-1, eval_after_first_stage=True, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=0, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:10:38 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:10:38 - INFO - transformers.data.processors.glue -   LOOKING AT glue_data/MRPC/train.tsv\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:10:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:10:42 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_train_bert-base-uncased_128_mrpc\n",
            "11/29/2021 03:10:43 - INFO - __main__ -   ***** Running training *****\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Num examples = 3668\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Num Epochs = 3\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "11/29/2021 03:10:43 - INFO - __main__ -     Total optimization steps = 1377\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A/content/drive/My Drive/NLP/DeeBERT/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/459 [00:00<03:21,  2.27it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:23,  2.24it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:25,  2.22it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:26,  2.21it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:26,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:26,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:05<03:25,  2.18it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:24,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:23,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:23,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:24,  2.17it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:23,  2.17it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:22,  2.18it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:21,  2.18it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:20,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:09<03:19,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:19,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:10<03:19,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:18,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:18,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:18,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:18,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:18,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:17,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:13<03:16,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:16,  2.18it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:14<03:15,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:15,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:15<03:15,  2.18it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:14,  2.18it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:13,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:13,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:13,  2.18it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:13,  2.18it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:12,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:18<03:11,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:11,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:19<03:10,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:10,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:20<03:09,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:08,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:21<03:08,  2.20it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:07,  2.20it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:07,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:22<03:06,  2.19it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  11% 50/459 [00:22<03:06,  2.19it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:23<03:06,  2.19it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:06,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:24<03:05,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:05,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:25<03:04,  2.18it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:04,  2.18it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:26<03:03,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:03,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:27<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:28<03:02,  2.17it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<03:02,  2.17it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:29<03:01,  2.18it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<03:00,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:30<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:59,  2.18it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:31<02:58,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:31<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:56,  2.20it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:32<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:56,  2.19it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:33<02:56,  2.19it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:56,  2.18it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:34<02:55,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:55,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:35<02:54,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:54,  2.18it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:36<02:54,  2.18it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:36<02:53,  2.19it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:37<02:53,  2.18it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:37<02:52,  2.19it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:51,  2.19it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:38<02:51,  2.18it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:51,  2.18it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:39<02:51,  2.18it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:50,  2.18it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:40<02:49,  2.18it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:40<02:49,  2.18it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:41<02:48,  2.19it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:41<02:48,  2.19it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:42<02:48,  2.18it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:42<02:47,  2.19it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:46,  2.20it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:43<02:45,  2.20it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:45,  2.20it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:44<02:44,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:44<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:45<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:46<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:46<02:42,  2.20it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:47<02:41,  2.20it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:47<02:41,  2.20it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:40,  2.20it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:48<02:40,  2.19it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:40,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:49<02:40,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:49<02:39,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:50<02:39,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:50<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:51<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:51<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:52<02:37,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:52<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:53<02:36,  2.18it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:53<02:36,  2.18it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:54<02:35,  2.18it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:54<02:35,  2.18it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:55<02:35,  2.18it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:55<02:34,  2.18it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:56<02:34,  2.18it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:56<02:33,  2.18it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:57<02:33,  2.18it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:57<02:32,  2.18it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:58<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:58<02:30,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:58<02:29,  2.20it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:59<02:29,  2.20it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:59<02:29,  2.20it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [01:00<02:28,  2.20it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [01:00<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:01<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:01<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:02<02:27,  2.18it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:02<02:26,  2.19it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:03<02:26,  2.19it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:03<02:25,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:03<02:26,  2.18it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:04<02:25,  2.18it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:04<02:25,  2.18it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:05<02:24,  2.18it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:05<02:24,  2.18it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:06<02:23,  2.18it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:06<02:23,  2.18it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:07<02:22,  2.18it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:07<02:21,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:08<02:21,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:08<02:20,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:08<02:20,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:09<02:20,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:09<02:19,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:10<02:19,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:10<02:18,  2.20it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:11<02:17,  2.20it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:11<02:17,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:12<02:17,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:12<02:17,  2.18it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:13<02:17,  2.18it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:13<02:16,  2.18it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:14<02:15,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:14<02:15,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:14<02:14,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:15<02:14,  2.18it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:15<02:14,  2.18it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:16<02:13,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:16<02:13,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:17<02:12,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:17<02:11,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:18<02:11,  2.18it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:18<02:11,  2.18it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:19<02:10,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:19<02:10,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:19<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:20<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:20<02:08,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:21<02:08,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:21<02:07,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:22<02:06,  2.20it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:22<02:06,  2.20it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:23<02:06,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:23<02:05,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:24<02:05,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:24<02:06,  2.17it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:24<02:05,  2.18it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:25<02:04,  2.18it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:25<02:04,  2.18it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:26<02:03,  2.19it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:26<02:02,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:27<02:02,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:27<02:01,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:28<02:01,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:28<02:01,  2.18it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:29<02:00,  2.19it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:29<02:00,  2.19it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:30<01:59,  2.19it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:30<01:59,  2.19it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:30<01:58,  2.19it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:31<01:58,  2.19it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:31<01:57,  2.19it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:32<01:57,  2.19it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:32<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:33<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:33<01:55,  2.20it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:34<01:54,  2.20it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:34<01:54,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:35<01:53,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:35<01:52,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:35<01:52,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:36<01:52,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:36<01:52,  2.20it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:37<01:51,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:37<01:51,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:38<01:51,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:38<01:50,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:39<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:39<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:40<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:40<01:48,  2.19it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:40<01:48,  2.19it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:41<01:48,  2.19it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:41<01:47,  2.19it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:42<01:47,  2.19it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:42<01:47,  2.18it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:43<01:46,  2.18it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:43<01:45,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:44<01:45,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:44<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:45<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:45<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:45<01:43,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:46<01:43,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:46<01:42,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:47<01:42,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:47<01:41,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:48<01:41,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:48<01:40,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:49<01:40,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:49<01:39,  2.20it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:50<01:39,  2.20it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:50<01:38,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:50<01:38,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:51<01:38,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:51<01:37,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:52<01:37,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:52<01:36,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:53<01:36,  2.20it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:53<01:35,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:54<01:35,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:54<01:34,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:55<01:34,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:55<01:34,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:55<01:33,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:56<01:33,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:56<01:32,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:57<01:32,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:57<01:31,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:58<01:31,  2.20it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:58<01:30,  2.20it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:59<01:30,  2.20it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:59<01:29,  2.20it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [02:00<01:28,  2.20it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [02:00<01:28,  2.20it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [02:00<01:28,  2.19it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [02:01<01:28,  2.19it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [02:01<01:28,  2.16it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:02<01:27,  2.18it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:02<01:27,  2.18it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:03<01:26,  2.19it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:03<01:26,  2.18it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:04<01:25,  2.19it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:04<01:24,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:05<01:24,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:05<01:24,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:06<01:23,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:06<01:23,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:06<01:22,  2.18it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:07<01:22,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:07<01:21,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:08<01:21,  2.20it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:08<01:20,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:09<01:20,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:09<01:19,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:10<01:19,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:10<01:18,  2.19it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:11<01:18,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:11<01:17,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:11<01:17,  2.20it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:12<01:17,  2.19it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:12<01:16,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:13<01:16,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:13<01:15,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:14<01:15,  2.19it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:14<01:14,  2.19it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:15<01:14,  2.19it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:15<01:13,  2.19it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:16<01:13,  2.19it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:16<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:16<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:17<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:17<01:11,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:18<01:11,  2.20it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:18<01:10,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:19<01:10,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:19<01:09,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:20<01:09,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:20<01:09,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:21<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:21<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:22<01:07,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:22<01:06,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:22<01:06,  2.20it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:23<01:05,  2.20it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:23<01:05,  2.20it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:24<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:24<01:04,  2.21it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:25<01:04,  2.20it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:25<01:03,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:26<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:26<01:02,  2.20it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:26<01:02,  2.20it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:27<01:01,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:27<01:01,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:28<01:00,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:28<01:00,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:29<01:00,  2.20it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:29<00:59,  2.19it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:30<00:59,  2.19it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:30<00:58,  2.19it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:31<00:58,  2.19it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:31<00:57,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:32<00:57,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:32<00:56,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:32<00:56,  2.20it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:33<00:56,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:33<00:55,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:34<00:55,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:34<00:54,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:35<00:54,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:35<00:53,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:36<00:53,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:36<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:37<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:37<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:37<00:51,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:38<00:50,  2.20it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:38<00:50,  2.20it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:39<00:50,  2.20it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:39<00:49,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:40<00:49,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:40<00:48,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:41<00:48,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:41<00:47,  2.20it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:42<00:47,  2.19it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:42<00:47,  2.19it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:42<00:46,  2.19it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:43<00:46,  2.19it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:43<00:45,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:44<00:45,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:44<00:44,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:45<00:44,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:45<00:43,  2.20it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:46<00:43,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:46<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:47<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:47<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:47<00:41,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:48<00:41,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:48<00:40,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:49<00:40,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:49<00:39,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:50<00:39,  2.20it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:50<00:38,  2.20it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:51<00:38,  2.20it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:51<00:37,  2.20it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:52<00:37,  2.19it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:52<00:37,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:52<00:36,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:53<00:36,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:53<00:35,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:54<00:35,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:54<00:34,  2.19it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:55<00:34,  2.18it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:55<00:33,  2.18it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:56<00:33,  2.18it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:56<00:32,  2.19it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:57<00:32,  2.18it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:57<00:32,  2.18it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:58<00:31,  2.18it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:58<00:31,  2.19it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:58<00:30,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:59<00:30,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:59<00:29,  2.20it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [03:00<00:29,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [03:00<00:28,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [03:01<00:28,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [03:01<00:27,  2.20it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [03:02<00:27,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [03:02<00:26,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [03:03<00:26,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:03<00:26,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:03<00:25,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:04<00:25,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:04<00:24,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:05<00:24,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:05<00:23,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:06<00:23,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:06<00:22,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:07<00:22,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:07<00:21,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:08<00:21,  2.18it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:08<00:21,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:08<00:20,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:09<00:20,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:09<00:19,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:10<00:19,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:10<00:18,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:11<00:18,  2.19it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:11<00:17,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:12<00:17,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:12<00:16,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:13<00:16,  2.20it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:13<00:15,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:13<00:15,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:14<00:14,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:14<00:14,  2.21it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:15<00:14,  2.20it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:15<00:13,  2.20it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:16<00:13,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:16<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:17<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:17<00:11,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:18<00:11,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:18<00:10,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:18<00:10,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:19<00:10,  2.20it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:19<00:09,  2.19it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:20<00:09,  2.19it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:20<00:08,  2.18it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:21<00:08,  2.18it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:21<00:07,  2.18it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:22<00:07,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:22<00:06,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:23<00:06,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:23<00:05,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:23<00:05,  2.19it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:24<00:05,  2.19it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:24<00:04,  2.19it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:25<00:04,  2.18it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:25<00:03,  2.18it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:26<00:03,  2.18it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:26<00:02,  2.19it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:27<00:02,  2.20it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:27<00:01,  2.20it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:28<00:01,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:28<00:00,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:29<00:00,  2.20it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:29<00:00,  2.19it/s]\n",
            "Epoch:  33% 1/3 [03:29<06:58, 209.28s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:29,  2.18it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:28,  2.19it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:27,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:27,  2.19it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:27,  2.19it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:26,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:24,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:05<03:24,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:24,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:23,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:22,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:22,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:22,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:21,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:21,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:20,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:09<03:19,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:19,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:10<03:19,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:18,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:17,  2.20it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:17,  2.20it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:17,  2.19it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:18,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:16,  2.19it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:13<03:16,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:15,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:14<03:15,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:14,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:15<03:14,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:14,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:13,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:13,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:12,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:12,  2.18it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:12,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:18<03:12,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:11,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:19<03:11,  2.18it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:10,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:20<03:10,  2.18it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:09,  2.18it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:21<03:07,  2.20it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:07,  2.20it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:06,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:22<03:06,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:22<03:05,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:23<03:05,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:05,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:24<03:04,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:04,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:25<03:04,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:04,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:26<03:03,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<03:03,  2.18it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:27<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<03:02,  2.18it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:28<03:01,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<03:01,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:29<03:00,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<03:00,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:30<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:59,  2.18it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:31<02:58,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:31<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:56,  2.20it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:32<02:56,  2.20it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:55,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:33<02:55,  2.20it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:54,  2.21it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:34<02:53,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:53,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:35<02:53,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:52,  2.21it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:36<02:52,  2.20it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:36<02:51,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:36<02:51,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:37<02:50,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:50,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:38<02:49,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:49,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:39<02:48,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:47,  2.22it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:40<02:47,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:40<02:47,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:41<02:46,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:41<02:46,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:41<02:45,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:42<02:45,  2.21it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:45,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:43<02:44,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:44,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:44<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:44<02:43,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:45<02:42,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:45<02:41,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:46<02:41,  2.21it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:46<02:41,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:47<02:40,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:39,  2.22it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:48<02:39,  2.21it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:38,  2.22it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:49<02:38,  2.21it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:49<02:38,  2.21it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:50<02:38,  2.20it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:50<02:37,  2.20it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:50<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:51<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:51<02:38,  2.18it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:52<02:37,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:53<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:53<02:35,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:54<02:35,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:54<02:34,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:55<02:34,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:55<02:33,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:55<02:33,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:56<02:32,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:56<02:31,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:57<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:57<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:58<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:58<02:30,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:59<02:30,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:59<02:29,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [01:00<02:29,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [01:00<02:28,  2.20it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:01<02:27,  2.20it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:01<02:27,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:01<02:26,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:02<02:26,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:02<02:26,  2.18it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:03<02:26,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:03<02:25,  2.20it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:04<02:25,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:04<02:24,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:05<02:24,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:05<02:23,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:06<02:23,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:06<02:22,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:06<02:21,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:07<02:21,  2.20it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:07<02:21,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:08<02:20,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:08<02:20,  2.20it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:09<02:19,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:09<02:19,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:10<02:19,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:10<02:18,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:11<02:18,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:11<02:17,  2.19it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:11<02:17,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:12<02:16,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:12<02:16,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:13<02:15,  2.20it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:13<02:14,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:14<02:14,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:14<02:13,  2.21it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:15<02:13,  2.20it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:15<02:13,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:16<02:12,  2.20it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:16<02:12,  2.20it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:16<02:12,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:17<02:12,  2.18it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:17<02:12,  2.18it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:18<02:11,  2.18it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:18<02:10,  2.18it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:19<02:10,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:19<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:20<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:20<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:21<02:08,  2.18it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:21<02:08,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:21<02:07,  2.18it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:22<02:07,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:22<02:06,  2.18it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:23<02:06,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:23<02:05,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:24<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:24<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:25<02:03,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:25<02:03,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:26<02:02,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:26<02:02,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:26<02:02,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:27<02:01,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:27<02:01,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:28<02:01,  2.19it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:28<02:00,  2.19it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:29<01:59,  2.20it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:29<01:59,  2.20it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:30<01:58,  2.20it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:30<01:59,  2.18it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:31<01:58,  2.19it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:31<01:57,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:32<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:32<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:32<01:56,  2.20it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:33<01:55,  2.19it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:33<01:55,  2.19it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:34<01:54,  2.19it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:34<01:54,  2.19it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:35<01:54,  2.19it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:35<01:53,  2.19it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:36<01:52,  2.20it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:36<01:51,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:37<01:51,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:37<01:50,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:37<01:50,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:38<01:49,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:38<01:49,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:39<01:49,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:39<01:49,  2.20it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:40<01:48,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:40<01:47,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:41<01:47,  2.20it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:41<01:47,  2.20it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:42<01:46,  2.20it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:42<01:46,  2.19it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:42<01:46,  2.19it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:43<01:46,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:43<01:45,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:44<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:44<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:45<01:44,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:45<01:43,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:46<01:43,  2.18it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:46<01:42,  2.18it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:47<01:42,  2.19it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:47<01:41,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:47<01:41,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:48<01:41,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:48<01:40,  2.19it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:49<01:39,  2.20it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:49<01:39,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:50<01:38,  2.20it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:50<01:38,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:51<01:38,  2.19it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:51<01:37,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:52<01:37,  2.20it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:52<01:36,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:52<01:36,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:53<01:36,  2.19it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:53<01:35,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:54<01:34,  2.19it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:54<01:34,  2.20it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:55<01:33,  2.20it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:55<01:33,  2.20it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:56<01:32,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:56<01:32,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:57<01:32,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:57<01:31,  2.19it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:57<01:31,  2.19it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:58<01:31,  2.18it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:58<01:30,  2.19it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:59<01:29,  2.20it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:59<01:29,  2.20it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [02:00<01:28,  2.20it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [02:00<01:28,  2.19it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [02:01<01:28,  2.19it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [02:01<01:27,  2.19it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:02<01:26,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:02<01:26,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:02<01:26,  2.20it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:03<01:25,  2.19it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:03<01:25,  2.19it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:04<01:24,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:04<01:24,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:05<01:23,  2.20it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:05<01:23,  2.19it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:06<01:23,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:06<01:22,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:07<01:21,  2.20it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:07<01:21,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:08<01:21,  2.19it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:08<01:20,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:08<01:20,  2.18it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:09<01:20,  2.18it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:09<01:19,  2.19it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:10<01:19,  2.18it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:10<01:18,  2.18it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:11<01:18,  2.18it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:11<01:17,  2.18it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:12<01:17,  2.18it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:12<01:16,  2.19it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:13<01:16,  2.19it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:13<01:15,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:13<01:15,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:14<01:14,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:14<01:14,  2.20it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:15<01:13,  2.20it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:15<01:13,  2.20it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:16<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:16<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:17<01:12,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:17<01:11,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:18<01:11,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:18<01:10,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:18<01:10,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:19<01:10,  2.18it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:19<01:09,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:20<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:20<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:21<01:08,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:21<01:07,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:22<01:07,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:22<01:06,  2.19it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:23<01:06,  2.19it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:23<01:05,  2.19it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:24<01:05,  2.18it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:24<01:05,  2.18it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:24<01:04,  2.19it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:25<01:03,  2.19it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:25<01:03,  2.20it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:26<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:26<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:27<01:01,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:27<01:01,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:28<01:00,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:28<01:00,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:28<00:59,  2.21it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:29<00:59,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:29<00:58,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:30<00:58,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:30<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:31<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:31<00:57,  2.20it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:32<00:56,  2.20it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:32<00:56,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:33<00:56,  2.19it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:33<00:55,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:33<00:55,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:34<00:54,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:34<00:54,  2.19it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:35<00:53,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:35<00:53,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:36<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:36<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:37<00:52,  2.19it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:37<00:51,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:38<00:51,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:38<00:50,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:39<00:50,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:39<00:49,  2.19it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:39<00:49,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:40<00:48,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:40<00:48,  2.19it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:41<00:47,  2.20it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:41<00:47,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:42<00:46,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:42<00:46,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:43<00:45,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:43<00:45,  2.20it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:44<00:45,  2.20it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:44<00:44,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:44<00:44,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:45<00:43,  2.19it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:45<00:43,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:46<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:46<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:47<00:42,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:47<00:41,  2.19it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:48<00:41,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:48<00:40,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:49<00:40,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:49<00:39,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:49<00:39,  2.19it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:50<00:38,  2.19it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:50<00:38,  2.19it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:51<00:37,  2.20it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:51<00:37,  2.19it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:52<00:36,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:52<00:36,  2.20it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:53<00:35,  2.20it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:53<00:35,  2.20it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:54<00:35,  2.19it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:54<00:34,  2.19it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:54<00:34,  2.18it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:55<00:33,  2.19it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:55<00:33,  2.19it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:56<00:32,  2.20it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:56<00:32,  2.20it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:57<00:31,  2.20it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:57<00:31,  2.20it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:58<00:30,  2.20it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:58<00:30,  2.20it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:59<00:30,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:59<00:29,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:59<00:29,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [03:00<00:28,  2.19it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [03:00<00:28,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [03:01<00:27,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [03:01<00:27,  2.20it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [03:02<00:26,  2.19it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [03:02<00:26,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:03<00:25,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:03<00:25,  2.20it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:04<00:25,  2.20it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:04<00:24,  2.19it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:04<00:24,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:05<00:23,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:05<00:23,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:06<00:22,  2.19it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:06<00:22,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:07<00:21,  2.18it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:07<00:21,  2.18it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:08<00:21,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:08<00:20,  2.19it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:09<00:20,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:09<00:19,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:10<00:19,  2.19it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:10<00:18,  2.17it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:10<00:18,  2.18it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:11<00:17,  2.18it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:11<00:17,  2.19it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:12<00:16,  2.19it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:12<00:16,  2.19it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:13<00:15,  2.19it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:13<00:15,  2.19it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:14<00:15,  2.19it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:14<00:14,  2.19it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:15<00:14,  2.21it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:15<00:13,  2.20it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:15<00:13,  2.20it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:16<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:16<00:12,  2.21it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:17<00:11,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:17<00:11,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:18<00:10,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:18<00:10,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:19<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:19<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:20<00:09,  2.20it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:20<00:08,  2.19it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:20<00:08,  2.19it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:21<00:07,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:21<00:07,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:22<00:06,  2.18it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:22<00:06,  2.18it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:23<00:05,  2.19it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:23<00:05,  2.19it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:24<00:05,  2.20it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:24<00:04,  2.20it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:25<00:04,  2.19it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:25<00:03,  2.18it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:25<00:03,  2.19it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:26<00:02,  2.19it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:26<00:02,  2.19it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:27<00:01,  2.19it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:27<00:01,  2.19it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:28<00:00,  2.19it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:28<00:00,  2.19it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:28<00:00,  2.20it/s]\n",
            "Epoch:  67% 2/3 [06:58<03:29, 209.10s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:26,  2.22it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:26,  2.21it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:26,  2.21it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<03:26,  2.21it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:02<03:26,  2.20it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:03<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<03:25,  2.20it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:04<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:04<03:25,  2.19it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:05<03:24,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:05<03:23,  2.20it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<03:23,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:06<03:22,  2.20it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:06<03:22,  2.19it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:07<03:22,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:07<03:22,  2.18it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:08<03:21,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:08<03:21,  2.19it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:09<03:21,  2.18it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:09<03:20,  2.18it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:10<03:19,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:10<03:18,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:10<03:18,  2.19it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:11<03:17,  2.20it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:11<03:18,  2.18it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:12<03:17,  2.19it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:12<03:16,  2.19it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:13<03:15,  2.20it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:13<03:14,  2.20it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:14<03:14,  2.20it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:14<03:15,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:15<03:14,  2.19it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:15<03:14,  2.18it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:15<03:14,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:16<03:13,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:16<03:12,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:17<03:12,  2.19it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:17<03:12,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:18<03:10,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:18<03:10,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:19<03:10,  2.19it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:19<03:09,  2.20it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:20<03:09,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:20<03:09,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:20<03:08,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:21<03:08,  2.19it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:21<03:07,  2.19it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:22<03:07,  2.19it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:22<03:06,  2.19it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:23<03:05,  2.20it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:23<03:05,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:24<03:04,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:24<03:04,  2.20it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:25<03:04,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:25<03:03,  2.19it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:25<03:02,  2.20it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:26<03:02,  2.20it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:26<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:27<03:02,  2.19it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:27<03:01,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:28<03:00,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:28<03:00,  2.20it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:29<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:29<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:30<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:30<02:59,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:31<02:58,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:31<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:31<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:32<02:57,  2.19it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:32<02:56,  2.19it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:33<02:56,  2.18it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:33<02:56,  2.18it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:34<02:56,  2.18it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:34<02:55,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:35<02:54,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:35<02:53,  2.19it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:36<02:52,  2.20it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:36<02:51,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:36<02:50,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:37<02:50,  2.21it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:37<02:50,  2.20it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:38<02:50,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:38<02:49,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:39<02:49,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:39<02:48,  2.21it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:40<02:48,  2.20it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:40<02:48,  2.19it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:41<02:48,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:41<02:47,  2.19it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:41<02:47,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:42<02:46,  2.20it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:42<02:46,  2.19it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:43<02:46,  2.19it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:43<02:45,  2.19it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:44<02:45,  2.19it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:44<02:45,  2.19it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:45<02:44,  2.19it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:45<02:44,  2.18it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:46<02:43,  2.19it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:46<02:43,  2.19it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:46<02:42,  2.19it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:47<02:43,  2.18it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:47<02:42,  2.18it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:48<02:41,  2.18it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:48<02:41,  2.18it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:49<02:40,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:49<02:39,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:50<02:39,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:50<02:38,  2.19it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:51<02:38,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:51<02:37,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:51<02:36,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:52<02:36,  2.20it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:52<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:53<02:36,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:53<02:35,  2.20it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:54<02:34,  2.19it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:54<02:34,  2.20it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:55<02:33,  2.20it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:55<02:33,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:56<02:33,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:56<02:33,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:57<02:32,  2.19it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:57<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:57<02:31,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:58<02:31,  2.18it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:58<02:30,  2.19it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:59<02:30,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:59<02:29,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [01:00<02:29,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [01:00<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [01:01<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [01:01<02:28,  2.19it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [01:02<02:27,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [01:02<02:26,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [01:02<02:26,  2.20it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [01:03<02:25,  2.19it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [01:03<02:25,  2.20it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [01:04<02:24,  2.20it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [01:04<02:24,  2.20it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [01:05<02:23,  2.20it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [01:05<02:23,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [01:06<02:23,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [01:06<02:23,  2.18it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [01:07<02:22,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [01:07<02:21,  2.19it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [01:07<02:21,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [01:08<02:21,  2.18it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [01:08<02:20,  2.19it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [01:09<02:20,  2.18it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:09<02:20,  2.17it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:10<02:20,  2.18it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:10<02:19,  2.18it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:11<02:18,  2.18it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:11<02:18,  2.18it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:12<02:17,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:12<02:16,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:12<02:16,  2.19it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:13<02:15,  2.20it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:13<02:15,  2.20it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:14<02:14,  2.20it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:14<02:14,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:15<02:13,  2.20it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:15<02:13,  2.19it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:16<02:13,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:16<02:12,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:17<02:12,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:17<02:11,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:18<02:11,  2.19it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:18<02:10,  2.20it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:18<02:10,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:19<02:10,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:19<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:20<02:09,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:20<02:08,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:21<02:08,  2.19it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:21<02:08,  2.18it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:22<02:07,  2.18it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:22<02:07,  2.18it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:23<02:06,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:23<02:05,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:23<02:05,  2.19it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:24<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:24<02:04,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:25<02:03,  2.21it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:25<02:02,  2.21it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:26<02:02,  2.20it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:26<02:01,  2.21it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:27<02:01,  2.20it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:27<02:01,  2.21it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:28<02:00,  2.21it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:28<02:00,  2.21it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:28<01:59,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:29<01:59,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:29<01:58,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:30<01:58,  2.21it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:30<01:57,  2.21it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:31<01:56,  2.22it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:31<01:56,  2.22it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:32<01:56,  2.21it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:32<01:55,  2.22it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:32<01:55,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:33<01:54,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:33<01:54,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:34<01:53,  2.21it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:34<01:53,  2.22it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:35<01:52,  2.22it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:35<01:52,  2.22it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:36<01:52,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:36<01:51,  2.21it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:37<01:50,  2.22it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:37<01:50,  2.22it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:37<01:50,  2.21it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:38<01:49,  2.22it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:38<01:49,  2.22it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:39<01:48,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:39<01:48,  2.22it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:40<01:48,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:40<01:47,  2.21it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:41<01:47,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:41<01:46,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:42<01:46,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:42<01:45,  2.22it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:42<01:45,  2.21it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:43<01:44,  2.21it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:43<01:44,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:44<01:43,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:44<01:43,  2.22it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:45<01:42,  2.21it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:45<01:42,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:46<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:46<01:41,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:46<01:40,  2.22it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:47<01:40,  2.22it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:47<01:40,  2.21it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:48<01:39,  2.22it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:48<01:39,  2.22it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:49<01:38,  2.22it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:49<01:38,  2.21it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:50<01:38,  2.21it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:50<01:37,  2.22it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:51<01:36,  2.22it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:51<01:36,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:51<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:52<01:35,  2.22it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:52<01:35,  2.21it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:53<01:34,  2.21it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:53<01:34,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:54<01:33,  2.21it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:54<01:33,  2.22it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:55<01:32,  2.22it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:55<01:32,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:56<01:32,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:56<01:31,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:56<01:31,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:57<01:30,  2.21it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:57<01:30,  2.22it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:58<01:29,  2.22it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:58<01:29,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:59<01:29,  2.21it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:59<01:28,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [02:00<01:28,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [02:00<01:27,  2.22it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [02:00<01:27,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [02:01<01:26,  2.21it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [02:01<01:26,  2.22it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [02:02<01:25,  2.22it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [02:02<01:25,  2.22it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [02:03<01:24,  2.22it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [02:03<01:24,  2.21it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [02:04<01:24,  2.21it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [02:04<01:23,  2.21it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [02:05<01:23,  2.22it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [02:05<01:22,  2.21it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [02:05<01:22,  2.21it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [02:06<01:21,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [02:06<01:21,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [02:07<01:20,  2.22it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [02:07<01:20,  2.21it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [02:08<01:20,  2.21it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [02:08<01:19,  2.21it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [02:09<01:19,  2.21it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [02:09<01:18,  2.21it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [02:10<01:18,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [02:10<01:17,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [02:10<01:17,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [02:11<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [02:11<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [02:12<01:16,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [02:12<01:15,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [02:13<01:15,  2.20it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [02:13<01:14,  2.21it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [02:14<01:14,  2.22it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [02:14<01:13,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [02:14<01:13,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [02:15<01:12,  2.21it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [02:15<01:12,  2.20it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [02:16<01:12,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [02:16<01:11,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [02:17<01:10,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [02:17<01:10,  2.21it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [02:18<01:09,  2.22it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:18<01:09,  2.22it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:19<01:08,  2.22it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:19<01:08,  2.22it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:19<01:08,  2.22it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:20<01:07,  2.21it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:20<01:07,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:21<01:06,  2.22it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:21<01:06,  2.21it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:22<01:06,  2.21it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:22<01:05,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:23<01:04,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:23<01:04,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:24<01:03,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:24<01:03,  2.22it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:24<01:03,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:25<01:02,  2.21it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:25<01:02,  2.22it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:26<01:01,  2.22it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:26<01:01,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:27<01:00,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:27<01:00,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:28<00:59,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:28<00:59,  2.22it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:28<00:59,  2.22it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:29<00:58,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:29<00:58,  2.22it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:30<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:30<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:31<00:57,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:31<00:56,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:32<00:56,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:32<00:55,  2.21it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:33<00:55,  2.22it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:33<00:54,  2.22it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:33<00:54,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:34<00:53,  2.21it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:34<00:53,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:35<00:52,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:35<00:52,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:36<00:52,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:36<00:51,  2.21it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:37<00:51,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:37<00:50,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:38<00:50,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:38<00:49,  2.21it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:38<00:49,  2.22it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:39<00:48,  2.22it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:39<00:48,  2.22it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:40<00:47,  2.22it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:40<00:47,  2.22it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:41<00:46,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:41<00:46,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:42<00:46,  2.22it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:42<00:45,  2.21it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:42<00:45,  2.21it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:43<00:44,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:43<00:44,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:44<00:43,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:44<00:43,  2.22it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:45<00:42,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:45<00:42,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:46<00:41,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:46<00:41,  2.22it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:47<00:41,  2.21it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:47<00:40,  2.21it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:47<00:40,  2.22it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:48<00:39,  2.22it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:48<00:39,  2.22it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:49<00:38,  2.22it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:49<00:38,  2.21it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:50<00:37,  2.21it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:50<00:37,  2.21it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:51<00:37,  2.21it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:51<00:36,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:52<00:36,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:52<00:35,  2.20it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:52<00:35,  2.21it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:53<00:35,  2.20it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:53<00:34,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:54<00:33,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:54<00:33,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:55<00:32,  2.21it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:55<00:32,  2.22it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:56<00:31,  2.22it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:56<00:31,  2.22it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:56<00:31,  2.22it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:57<00:30,  2.21it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:57<00:30,  2.22it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:58<00:29,  2.22it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:58<00:29,  2.22it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:59<00:28,  2.21it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:59<00:28,  2.22it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [03:00<00:27,  2.22it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [03:00<00:27,  2.22it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [03:01<00:27,  2.22it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [03:01<00:26,  2.21it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [03:01<00:26,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [03:02<00:25,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [03:02<00:25,  2.22it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [03:03<00:24,  2.21it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [03:03<00:24,  2.21it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [03:04<00:23,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [03:04<00:23,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [03:05<00:22,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [03:05<00:22,  2.22it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [03:06<00:22,  2.22it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [03:06<00:21,  2.22it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [03:06<00:21,  2.22it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [03:07<00:20,  2.22it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [03:07<00:20,  2.21it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [03:08<00:19,  2.22it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [03:08<00:19,  2.21it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [03:09<00:18,  2.21it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [03:09<00:18,  2.22it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [03:10<00:18,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [03:10<00:17,  2.21it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [03:10<00:17,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [03:11<00:16,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [03:11<00:16,  2.22it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [03:12<00:15,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [03:12<00:15,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [03:13<00:14,  2.21it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [03:13<00:14,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [03:14<00:13,  2.22it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [03:14<00:13,  2.22it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [03:15<00:13,  2.22it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [03:15<00:12,  2.22it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [03:15<00:12,  2.22it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [03:16<00:11,  2.22it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [03:16<00:11,  2.22it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [03:17<00:10,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [03:17<00:10,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [03:18<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [03:18<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [03:19<00:09,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [03:19<00:08,  2.21it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [03:19<00:08,  2.22it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [03:20<00:07,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [03:20<00:07,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [03:21<00:06,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [03:21<00:06,  2.22it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [03:22<00:05,  2.21it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [03:22<00:05,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [03:23<00:04,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [03:23<00:04,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [03:24<00:04,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [03:24<00:03,  2.21it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [03:24<00:03,  2.21it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [03:25<00:02,  2.22it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [03:25<00:02,  2.21it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [03:26<00:01,  2.22it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [03:26<00:01,  2.21it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:27<00:00,  2.22it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:27<00:00,  2.21it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:27<00:00,  2.21it/s]\n",
            "Epoch: 100% 3/3 [10:26<00:00, 208.73s/it]\n",
            "11/29/2021 03:21:09 - INFO - __main__ -    global_step = 1377, average loss = 0.30821044915342216\n",
            "11/29/2021 03:21:09 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:21:09 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:21:09 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc\n",
            "11/29/2021 03:21:10 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:21:10 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:21:10 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.58it/s]\n",
            "Eval time: 13.341790437698364\n",
            "11/29/2021 03:21:23 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "11/29/2021 03:21:23 - INFO - __main__ -   ***** Running training *****\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Num examples = 3668\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Num Epochs = 3\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "11/29/2021 03:21:23 - INFO - __main__ -     Total optimization steps = 1377\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<02:51,  2.67it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<02:54,  2.62it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<02:57,  2.57it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<02:55,  2.59it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:01<02:56,  2.57it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<02:57,  2.55it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:02<02:56,  2.57it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<02:57,  2.54it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:03<02:56,  2.56it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:03<02:55,  2.55it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:04<02:54,  2.56it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:04<02:54,  2.56it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<02:53,  2.57it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:05<02:53,  2.57it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:05<02:52,  2.58it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:06<02:52,  2.56it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:06<02:51,  2.57it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:07<02:51,  2.57it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:07<02:54,  2.52it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:07<02:52,  2.55it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:08<02:51,  2.56it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:08<02:50,  2.56it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:08<02:49,  2.57it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:09<02:49,  2.56it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:09<02:48,  2.57it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:10<02:48,  2.57it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:10<02:47,  2.57it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:10<02:48,  2.56it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:11<02:46,  2.58it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:11<02:47,  2.57it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:12<02:46,  2.57it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:12<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:12<02:46,  2.57it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:13<02:46,  2.55it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:13<02:45,  2.56it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:14<02:45,  2.56it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:14<02:45,  2.56it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:14<02:44,  2.56it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:15<02:43,  2.57it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:15<02:42,  2.57it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:15<02:42,  2.57it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:16<02:42,  2.57it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:16<02:42,  2.57it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:17<02:41,  2.56it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:17<02:41,  2.57it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:17<02:40,  2.57it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:18<02:40,  2.57it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:18<02:40,  2.57it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:19<02:39,  2.57it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  11% 50/459 [00:19<02:40,  2.54it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:19<02:39,  2.55it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:20<02:39,  2.56it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:20<02:38,  2.57it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:21<02:37,  2.57it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:21<02:37,  2.57it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:21<02:36,  2.57it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:22<02:36,  2.57it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:22<02:35,  2.57it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:23<02:35,  2.57it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:23<02:35,  2.56it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:23<02:35,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:24<02:35,  2.56it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:24<02:34,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:24<02:33,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:25<02:33,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:25<02:33,  2.56it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:26<02:32,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:26<02:32,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:26<02:31,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:27<02:31,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:27<02:30,  2.58it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:28<02:30,  2.57it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:28<02:30,  2.56it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:28<02:29,  2.57it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:29<02:29,  2.57it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:29<02:28,  2.57it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:30<02:29,  2.55it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:30<02:28,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:30<02:28,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:31<02:29,  2.53it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:31<02:28,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:31<02:28,  2.55it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:32<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:32<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:33<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:33<02:25,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:33<02:25,  2.55it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:34<02:25,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:34<02:24,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:35<02:23,  2.57it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:35<02:23,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:35<02:24,  2.55it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:36<02:23,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:36<02:22,  2.56it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:37<02:21,  2.56it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:37<02:21,  2.57it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:37<02:21,  2.56it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:38<02:21,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:38<02:20,  2.57it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:39<02:19,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:39<02:19,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:39<02:18,  2.58it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:40<02:18,  2.57it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:40<02:18,  2.56it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:40<02:19,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:41<02:18,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:41<02:17,  2.56it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:42<02:16,  2.56it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:42<02:17,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:42<02:15,  2.57it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:43<02:15,  2.58it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:43<02:14,  2.58it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:44<02:14,  2.57it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:44<02:14,  2.57it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:44<02:14,  2.56it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:45<02:13,  2.56it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:45<02:13,  2.57it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:46<02:13,  2.55it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:46<02:12,  2.57it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:46<02:11,  2.58it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:47<02:11,  2.57it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:47<02:11,  2.57it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:47<02:11,  2.56it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:48<02:10,  2.57it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:48<02:10,  2.57it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:49<02:09,  2.57it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:49<02:09,  2.57it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:49<02:09,  2.56it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:50<02:08,  2.57it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:50<02:08,  2.56it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:51<02:07,  2.57it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:51<02:07,  2.56it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [00:51<02:07,  2.57it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [00:52<02:06,  2.57it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [00:52<02:05,  2.58it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [00:53<02:05,  2.58it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [00:53<02:05,  2.57it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [00:53<02:04,  2.58it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [00:54<02:04,  2.57it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [00:54<02:04,  2.57it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [00:54<02:03,  2.57it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [00:55<02:02,  2.58it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [00:55<02:02,  2.58it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [00:56<02:02,  2.58it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [00:56<02:02,  2.57it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [00:56<02:01,  2.57it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [00:57<02:01,  2.57it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [00:57<02:01,  2.57it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [00:58<02:00,  2.57it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [00:58<02:00,  2.57it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [00:58<01:59,  2.57it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [00:59<02:00,  2.55it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [00:59<01:59,  2.56it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:00<01:59,  2.54it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:00<01:59,  2.55it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:00<01:58,  2.56it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:01<01:57,  2.56it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:01<01:57,  2.57it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:01<01:56,  2.57it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:02<01:56,  2.58it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:02<01:56,  2.57it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:03<01:55,  2.57it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:03<01:55,  2.56it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:03<01:55,  2.56it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:04<01:55,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:04<01:55,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:05<01:54,  2.55it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:05<01:53,  2.56it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:05<01:52,  2.57it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:06<01:52,  2.56it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:06<01:52,  2.55it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:07<01:51,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:07<01:51,  2.56it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:07<01:51,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:08<01:50,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:08<01:50,  2.57it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:09<01:49,  2.57it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:09<01:49,  2.58it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:09<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:10<01:48,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:10<01:47,  2.57it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:11<01:47,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:11<01:47,  2.57it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:12<01:46,  2.57it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:12<01:46,  2.57it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:12<01:45,  2.58it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:13<01:45,  2.58it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:13<01:45,  2.57it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:14<01:44,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:14<01:44,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:14<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:15<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:15<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:16<01:42,  2.57it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:16<01:42,  2.57it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:16<01:41,  2.57it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:17<01:41,  2.57it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:17<01:41,  2.57it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:17<01:40,  2.57it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:18<01:39,  2.58it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:18<01:39,  2.58it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:19<01:39,  2.58it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:19<01:39,  2.57it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:19<01:38,  2.57it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:20<01:38,  2.57it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:20<01:37,  2.58it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:21<01:37,  2.57it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:21<01:36,  2.59it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:21<01:36,  2.58it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:22<01:36,  2.57it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:22<01:36,  2.55it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:23<01:36,  2.55it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:23<01:35,  2.56it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:23<01:35,  2.56it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:24<01:34,  2.56it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:24<01:34,  2.57it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:24<01:33,  2.57it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:25<01:33,  2.57it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:25<01:33,  2.56it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:26<01:32,  2.57it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:26<01:32,  2.57it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:26<01:31,  2.57it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:27<01:31,  2.57it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:27<01:31,  2.55it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:28<01:31,  2.55it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:28<01:30,  2.56it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:28<01:29,  2.57it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:29<01:29,  2.56it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:29<01:29,  2.57it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:30<01:29,  2.55it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:30<01:28,  2.56it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:30<01:28,  2.56it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:31<01:27,  2.57it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:31<01:27,  2.57it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:31<01:26,  2.57it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:32<01:26,  2.58it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:32<01:26,  2.57it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:33<01:25,  2.57it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:33<01:25,  2.56it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:33<01:24,  2.57it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:34<01:24,  2.57it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:34<01:24,  2.57it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:35<01:23,  2.58it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:35<01:23,  2.57it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:35<01:23,  2.55it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:36<01:23,  2.55it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:36<01:22,  2.56it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:37<01:22,  2.55it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:37<01:21,  2.55it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:37<01:21,  2.56it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:38<01:20,  2.57it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:38<01:20,  2.57it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:39<01:20,  2.56it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:39<01:19,  2.57it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:39<01:19,  2.56it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:40<01:19,  2.55it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:40<01:18,  2.55it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:40<01:18,  2.55it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:41<01:17,  2.56it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:41<01:17,  2.55it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:42<01:16,  2.56it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:42<01:16,  2.57it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:42<01:15,  2.57it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [01:43<01:15,  2.57it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [01:43<01:15,  2.57it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [01:44<01:14,  2.57it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [01:44<01:14,  2.57it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [01:44<01:13,  2.57it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [01:45<01:13,  2.56it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [01:45<01:13,  2.57it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [01:46<01:13,  2.55it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [01:46<01:12,  2.56it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [01:46<01:12,  2.55it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [01:47<01:11,  2.56it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [01:47<01:11,  2.56it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [01:47<01:10,  2.56it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [01:48<01:10,  2.57it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [01:48<01:10,  2.57it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [01:49<01:09,  2.57it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [01:49<01:09,  2.56it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [01:49<01:08,  2.57it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [01:50<01:08,  2.56it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [01:50<01:08,  2.56it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [01:51<01:08,  2.54it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [01:51<01:07,  2.56it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [01:51<01:07,  2.57it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [01:52<01:06,  2.57it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [01:52<01:06,  2.57it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [01:53<01:05,  2.56it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [01:53<01:05,  2.57it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [01:53<01:05,  2.57it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [01:54<01:04,  2.57it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [01:54<01:04,  2.56it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [01:55<01:03,  2.57it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [01:55<01:03,  2.57it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [01:55<01:03,  2.57it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [01:56<01:02,  2.57it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [01:56<01:02,  2.56it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [01:56<01:01,  2.57it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [01:57<01:01,  2.57it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [01:57<01:01,  2.57it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [01:58<01:00,  2.56it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [01:58<01:00,  2.56it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [01:58<00:59,  2.57it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [01:59<00:59,  2.57it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [01:59<00:59,  2.57it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:00<00:58,  2.57it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:00<00:58,  2.57it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:00<00:57,  2.58it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:01<00:57,  2.58it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:01<00:57,  2.57it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:02<00:56,  2.57it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:02<00:56,  2.58it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:02<00:55,  2.57it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:03<00:55,  2.58it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:03<00:55,  2.57it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:03<00:54,  2.57it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:04<00:54,  2.57it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:04<00:53,  2.58it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:05<00:53,  2.57it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:05<00:53,  2.57it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:05<00:52,  2.57it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:06<00:52,  2.57it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:06<00:52,  2.56it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:07<00:51,  2.57it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:07<00:51,  2.57it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:07<00:51,  2.56it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:08<00:50,  2.57it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:08<00:50,  2.57it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:09<00:50,  2.56it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:09<00:49,  2.54it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:09<00:49,  2.55it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:10<00:48,  2.55it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:10<00:48,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:10<00:48,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:11<00:47,  2.56it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:11<00:47,  2.57it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:12<00:46,  2.58it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:12<00:46,  2.56it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:12<00:46,  2.56it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:13<00:45,  2.56it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:13<00:45,  2.56it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:14<00:44,  2.57it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:14<00:44,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:14<00:44,  2.56it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:15<00:43,  2.57it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:15<00:43,  2.57it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:16<00:42,  2.57it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:16<00:42,  2.56it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:16<00:42,  2.56it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:17<00:41,  2.57it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:17<00:41,  2.57it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:17<00:40,  2.58it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:18<00:40,  2.58it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:18<00:39,  2.58it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:19<00:39,  2.57it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:19<00:39,  2.57it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:19<00:38,  2.57it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:20<00:38,  2.56it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:20<00:38,  2.56it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:21<00:38,  2.55it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:21<00:37,  2.56it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:21<00:37,  2.56it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:22<00:36,  2.55it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:22<00:36,  2.55it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:23<00:36,  2.55it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:23<00:35,  2.55it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:23<00:35,  2.55it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:24<00:35,  2.54it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:24<00:34,  2.52it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:25<00:34,  2.53it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:25<00:34,  2.53it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:25<00:33,  2.53it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:26<00:33,  2.53it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:26<00:32,  2.54it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:27<00:32,  2.55it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:27<00:31,  2.55it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:27<00:31,  2.55it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:28<00:31,  2.54it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:28<00:30,  2.54it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:28<00:30,  2.54it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:29<00:29,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:29<00:29,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:30<00:29,  2.53it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:30<00:28,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:30<00:28,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:31<00:27,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:31<00:27,  2.55it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:32<00:27,  2.53it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:32<00:26,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:32<00:26,  2.55it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:33<00:26,  2.53it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:33<00:25,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:34<00:25,  2.53it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:34<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:34<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:35<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [02:35<00:23,  2.55it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [02:36<00:23,  2.55it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [02:36<00:22,  2.55it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [02:36<00:22,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [02:37<00:22,  2.54it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [02:37<00:21,  2.54it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [02:38<00:21,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [02:38<00:20,  2.53it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [02:38<00:20,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [02:39<00:20,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [02:39<00:19,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [02:40<00:19,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [02:40<00:18,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [02:40<00:18,  2.53it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [02:41<00:18,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [02:41<00:17,  2.55it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [02:41<00:17,  2.54it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [02:42<00:16,  2.54it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [02:42<00:16,  2.55it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [02:43<00:16,  2.54it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [02:43<00:15,  2.54it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [02:43<00:15,  2.52it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [02:44<00:15,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [02:44<00:14,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [02:45<00:14,  2.52it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [02:45<00:13,  2.53it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [02:45<00:13,  2.53it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [02:46<00:13,  2.52it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [02:46<00:12,  2.52it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [02:47<00:12,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [02:47<00:11,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [02:47<00:11,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [02:48<00:10,  2.55it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [02:48<00:10,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [02:49<00:10,  2.52it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [02:49<00:09,  2.54it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [02:49<00:09,  2.53it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [02:50<00:09,  2.51it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [02:50<00:08,  2.52it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [02:51<00:08,  2.52it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [02:51<00:07,  2.53it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [02:51<00:07,  2.54it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [02:52<00:07,  2.53it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [02:52<00:06,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [02:53<00:06,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [02:53<00:05,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [02:53<00:05,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [02:54<00:05,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [02:54<00:04,  2.54it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [02:55<00:04,  2.55it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [02:55<00:03,  2.54it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [02:55<00:03,  2.54it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [02:56<00:03,  2.51it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [02:56<00:02,  2.53it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [02:56<00:02,  2.54it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [02:57<00:01,  2.54it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [02:57<00:01,  2.54it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [02:58<00:01,  2.53it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [02:58<00:00,  2.55it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [02:58<00:00,  2.55it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [02:59<00:00,  2.56it/s]\n",
            "Epoch:  33% 1/3 [02:59<05:58, 179.17s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:03,  2.49it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<02:59,  2.54it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:00,  2.53it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<02:58,  2.54it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:01<02:58,  2.54it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<02:57,  2.55it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:02<02:57,  2.55it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<02:58,  2.52it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:03<02:57,  2.53it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:03<02:57,  2.53it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:04<02:56,  2.54it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:04<02:56,  2.54it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<02:55,  2.54it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:05<02:55,  2.53it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:05<02:54,  2.55it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:06<02:54,  2.54it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:06<02:54,  2.53it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:07<02:53,  2.54it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:07<02:53,  2.53it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:07<02:53,  2.53it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:08<02:52,  2.53it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:08<02:52,  2.53it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:09<02:52,  2.53it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:09<02:51,  2.54it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:09<02:51,  2.53it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:10<02:52,  2.51it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:10<02:50,  2.53it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:11<02:50,  2.53it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:11<02:50,  2.52it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:11<02:48,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:12<02:48,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:12<02:49,  2.53it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:13<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:13<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:13<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:14<02:45,  2.55it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:14<02:45,  2.55it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:14<02:44,  2.56it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:15<02:45,  2.54it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:15<02:43,  2.56it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:16<02:43,  2.55it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:16<02:42,  2.56it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:16<02:43,  2.55it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:17<02:42,  2.55it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:17<02:42,  2.55it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:18<02:41,  2.56it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:18<02:41,  2.56it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:18<02:41,  2.55it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:19<02:40,  2.55it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:19<02:40,  2.55it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:20<02:39,  2.55it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:20<02:39,  2.55it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:20<02:39,  2.55it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:21<02:39,  2.54it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:21<02:39,  2.54it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:22<02:38,  2.54it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:22<02:39,  2.52it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:22<02:39,  2.52it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:23<02:39,  2.51it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:23<02:38,  2.51it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:24<02:38,  2.52it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:24<02:37,  2.52it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:24<02:36,  2.52it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:25<02:37,  2.51it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:25<02:36,  2.52it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:26<02:35,  2.52it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:26<02:34,  2.53it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:26<02:34,  2.53it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:27<02:33,  2.54it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:27<02:33,  2.54it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:27<02:33,  2.53it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:28<02:32,  2.54it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:28<02:32,  2.53it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:29<02:31,  2.53it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:29<02:32,  2.51it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:29<02:32,  2.51it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:30<02:31,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:30<02:30,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:31<02:29,  2.54it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:31<02:28,  2.55it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:31<02:29,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:32<02:28,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:32<02:28,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:33<02:27,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:33<02:27,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:33<02:26,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:34<02:26,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:34<02:26,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:35<02:25,  2.54it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:35<02:25,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:35<02:25,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:36<02:25,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:36<02:26,  2.49it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:37<02:25,  2.51it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:37<02:25,  2.51it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:37<02:24,  2.52it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:38<02:23,  2.53it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:38<02:23,  2.52it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:39<02:22,  2.53it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:39<02:21,  2.53it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:39<02:21,  2.53it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:40<02:20,  2.54it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:40<02:20,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:41<02:20,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:41<02:19,  2.54it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:41<02:19,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:42<02:19,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:42<02:18,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:43<02:18,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:43<02:17,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:43<02:18,  2.52it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:44<02:17,  2.52it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:44<02:16,  2.53it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:45<02:16,  2.53it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:45<02:15,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:45<02:15,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:46<02:14,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:46<02:14,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:46<02:13,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:47<02:13,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:47<02:13,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:48<02:13,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:48<02:12,  2.54it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:48<02:12,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:49<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:49<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:50<02:10,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:50<02:10,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:50<02:10,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:51<02:09,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:51<02:08,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:52<02:07,  2.56it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [00:52<02:07,  2.55it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [00:52<02:07,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [00:53<02:08,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [00:53<02:07,  2.54it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [00:54<02:07,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [00:54<02:06,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [00:54<02:06,  2.54it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [00:55<02:05,  2.54it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [00:55<02:04,  2.55it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [00:56<02:05,  2.52it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [00:56<02:04,  2.54it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [00:56<02:03,  2.54it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [00:57<02:03,  2.55it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [00:57<02:02,  2.55it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [00:57<02:02,  2.55it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [00:58<02:02,  2.54it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [00:58<02:02,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [00:59<02:01,  2.53it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [00:59<02:01,  2.53it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [00:59<02:01,  2.53it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:00<02:00,  2.53it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:00<02:00,  2.54it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:01<01:59,  2.53it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:01<01:59,  2.53it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:01<01:58,  2.54it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:02<01:58,  2.55it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:02<01:58,  2.54it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:03<01:57,  2.55it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:03<01:57,  2.53it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:03<01:57,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:04<01:56,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:04<01:56,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:05<01:55,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:05<01:55,  2.54it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:05<01:55,  2.54it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:06<01:54,  2.53it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:06<01:54,  2.53it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:07<01:54,  2.53it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:07<01:53,  2.54it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:07<01:52,  2.55it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:08<01:53,  2.53it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:08<01:51,  2.54it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:09<01:51,  2.54it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:09<01:51,  2.55it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:09<01:50,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:11<01:48,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:11<01:48,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:12<01:47,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:12<01:47,  2.55it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:12<01:47,  2.55it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:13<01:47,  2.54it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:13<01:47,  2.54it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:14<01:46,  2.54it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:14<01:46,  2.54it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:14<01:45,  2.54it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:15<01:45,  2.54it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:15<01:45,  2.54it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:16<01:44,  2.54it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:16<01:44,  2.55it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:16<01:43,  2.54it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:17<01:43,  2.54it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:17<01:43,  2.54it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:18<01:42,  2.54it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:18<01:42,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:18<01:41,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:19<01:41,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:19<01:41,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:20<01:40,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:20<01:41,  2.52it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:20<01:40,  2.54it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:21<01:39,  2.54it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:21<01:39,  2.54it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:22<01:38,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:22<01:38,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:22<01:38,  2.53it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:23<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:23<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:23<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:24<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:24<01:36,  2.53it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:25<01:35,  2.54it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:25<01:35,  2.54it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:25<01:34,  2.55it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:26<01:34,  2.55it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:26<01:33,  2.55it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:27<01:33,  2.54it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:27<01:33,  2.54it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:27<01:32,  2.54it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:28<01:32,  2.54it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:28<01:32,  2.53it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:29<01:31,  2.55it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:29<01:31,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:29<01:31,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:30<01:30,  2.55it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:30<01:30,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:31<01:29,  2.53it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:31<01:29,  2.54it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:31<01:29,  2.53it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:32<01:29,  2.52it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:32<01:29,  2.51it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:33<01:28,  2.52it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:33<01:27,  2.52it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:33<01:27,  2.52it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:34<01:27,  2.52it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:34<01:26,  2.52it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:35<01:26,  2.53it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:35<01:25,  2.53it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:35<01:25,  2.53it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:36<01:24,  2.53it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:36<01:24,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:36<01:23,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:37<01:23,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:37<01:22,  2.55it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:38<01:22,  2.55it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:38<01:22,  2.54it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:38<01:21,  2.55it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:39<01:21,  2.55it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:39<01:21,  2.53it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:40<01:20,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:40<01:20,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:40<01:20,  2.53it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:41<01:19,  2.53it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:41<01:19,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:42<01:18,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:42<01:18,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:42<01:17,  2.55it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:43<01:17,  2.53it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:43<01:17,  2.54it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:44<01:16,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [01:44<01:16,  2.54it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [01:44<01:16,  2.53it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [01:45<01:15,  2.54it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [01:45<01:15,  2.53it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [01:46<01:15,  2.52it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [01:46<01:14,  2.52it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [01:46<01:14,  2.53it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [01:47<01:13,  2.53it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [01:47<01:13,  2.53it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [01:48<01:12,  2.54it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [01:48<01:12,  2.55it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [01:48<01:12,  2.53it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [01:49<01:11,  2.54it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [01:49<01:11,  2.53it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [01:50<01:11,  2.51it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [01:50<01:11,  2.52it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [01:50<01:10,  2.53it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [01:51<01:09,  2.53it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [01:51<01:09,  2.54it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [01:51<01:09,  2.52it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [01:52<01:08,  2.53it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [01:52<01:08,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [01:53<01:08,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [01:53<01:07,  2.54it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [01:53<01:06,  2.54it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [01:54<01:06,  2.54it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [01:54<01:06,  2.54it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [01:55<01:06,  2.53it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [01:55<01:05,  2.53it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [01:55<01:05,  2.53it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [01:56<01:05,  2.52it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [01:56<01:04,  2.53it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [01:57<01:04,  2.53it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [01:57<01:03,  2.54it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [01:57<01:03,  2.54it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [01:58<01:02,  2.53it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [01:58<01:02,  2.53it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [01:59<01:02,  2.51it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [01:59<01:01,  2.52it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [01:59<01:01,  2.52it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:00<01:00,  2.53it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:00<01:00,  2.53it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:01<00:59,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:01<00:59,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:01<00:59,  2.53it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:02<00:58,  2.54it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:02<00:58,  2.53it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:03<00:57,  2.55it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:03<00:57,  2.56it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:03<00:56,  2.55it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:04<00:56,  2.56it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:04<00:55,  2.56it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:04<00:55,  2.56it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:05<00:55,  2.55it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:05<00:54,  2.55it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:06<00:54,  2.54it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:06<00:54,  2.55it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:06<00:53,  2.56it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:07<00:53,  2.55it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:07<00:53,  2.54it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:08<00:52,  2.54it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:08<00:52,  2.55it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:08<00:52,  2.53it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:09<00:51,  2.53it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:09<00:51,  2.55it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:10<00:50,  2.53it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:10<00:50,  2.53it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:10<00:50,  2.53it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:11<00:50,  2.50it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:11<00:49,  2.52it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:12<00:49,  2.52it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:12<00:48,  2.52it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:12<00:48,  2.53it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:13<00:47,  2.53it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:13<00:47,  2.53it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:14<00:47,  2.53it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:14<00:46,  2.54it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:14<00:46,  2.53it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:15<00:45,  2.53it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:15<00:45,  2.54it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:16<00:44,  2.54it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:16<00:44,  2.55it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:16<00:44,  2.53it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:17<00:43,  2.53it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:17<00:43,  2.54it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:17<00:42,  2.54it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:18<00:42,  2.52it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:18<00:42,  2.52it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:19<00:41,  2.53it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:19<00:41,  2.54it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:19<00:40,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:20<00:40,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:20<00:39,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:21<00:39,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:21<00:39,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:21<00:39,  2.53it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:22<00:38,  2.52it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:22<00:38,  2.53it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:23<00:38,  2.52it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:23<00:37,  2.53it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:23<00:36,  2.55it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:24<00:36,  2.53it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:24<00:36,  2.53it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:25<00:36,  2.53it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:25<00:35,  2.53it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:25<00:35,  2.52it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:26<00:34,  2.54it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:26<00:34,  2.51it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:27<00:33,  2.53it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:27<00:33,  2.54it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:27<00:33,  2.53it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:28<00:32,  2.53it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:28<00:32,  2.53it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:29<00:31,  2.53it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:29<00:31,  2.54it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:29<00:31,  2.53it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:30<00:30,  2.53it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:30<00:30,  2.53it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:31<00:29,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:31<00:29,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:31<00:29,  2.54it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:32<00:28,  2.55it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:32<00:28,  2.55it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:32<00:27,  2.56it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:33<00:27,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:33<00:27,  2.55it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:34<00:26,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:34<00:26,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:34<00:25,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:35<00:25,  2.52it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:35<00:25,  2.53it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:36<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:36<00:24,  2.53it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:36<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [02:37<00:23,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [02:37<00:23,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [02:38<00:22,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [02:38<00:22,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [02:38<00:22,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [02:39<00:21,  2.54it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [02:39<00:21,  2.53it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [02:40<00:21,  2.52it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [02:40<00:20,  2.53it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [02:40<00:20,  2.53it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [02:41<00:19,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [02:41<00:19,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [02:42<00:18,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [02:42<00:18,  2.53it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [02:42<00:18,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [02:43<00:17,  2.55it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [02:43<00:17,  2.55it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [02:44<00:16,  2.55it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [02:44<00:16,  2.53it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [02:44<00:16,  2.53it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [02:45<00:15,  2.55it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [02:45<00:15,  2.55it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [02:45<00:14,  2.55it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [02:46<00:14,  2.56it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [02:46<00:14,  2.55it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [02:47<00:13,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [02:47<00:13,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [02:47<00:12,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [02:48<00:12,  2.53it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [02:48<00:12,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [02:49<00:11,  2.53it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [02:49<00:11,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [02:49<00:11,  2.51it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [02:50<00:10,  2.52it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [02:50<00:10,  2.52it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [02:51<00:09,  2.52it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [02:51<00:09,  2.52it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [02:51<00:09,  2.54it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [02:52<00:08,  2.53it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [02:52<00:08,  2.54it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [02:53<00:07,  2.54it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [02:53<00:07,  2.54it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [02:53<00:07,  2.54it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [02:54<00:06,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [02:54<00:06,  2.54it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [02:55<00:05,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [02:55<00:05,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [02:55<00:05,  2.52it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [02:56<00:04,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [02:56<00:04,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [02:57<00:03,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [02:57<00:03,  2.55it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [02:57<00:03,  2.55it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [02:58<00:02,  2.56it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [02:58<00:02,  2.57it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [02:58<00:01,  2.57it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [02:59<00:01,  2.56it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [02:59<00:01,  2.56it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [03:00<00:00,  2.55it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:00<00:00,  2.56it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:00<00:00,  2.54it/s]\n",
            "Epoch:  67% 2/3 [05:59<03:00, 180.12s/it]\n",
            "Iteration:   0% 0/459 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/459 [00:00<03:02,  2.51it/s]\u001b[A\n",
            "Iteration:   0% 2/459 [00:00<03:01,  2.52it/s]\u001b[A\n",
            "Iteration:   1% 3/459 [00:01<03:00,  2.53it/s]\u001b[A\n",
            "Iteration:   1% 4/459 [00:01<02:59,  2.53it/s]\u001b[A\n",
            "Iteration:   1% 5/459 [00:01<02:58,  2.54it/s]\u001b[A\n",
            "Iteration:   1% 6/459 [00:02<03:00,  2.51it/s]\u001b[A\n",
            "Iteration:   2% 7/459 [00:02<02:59,  2.52it/s]\u001b[A\n",
            "Iteration:   2% 8/459 [00:03<02:58,  2.52it/s]\u001b[A\n",
            "Iteration:   2% 9/459 [00:03<02:58,  2.52it/s]\u001b[A\n",
            "Iteration:   2% 10/459 [00:03<02:57,  2.53it/s]\u001b[A\n",
            "Iteration:   2% 11/459 [00:04<02:57,  2.52it/s]\u001b[A\n",
            "Iteration:   3% 12/459 [00:04<02:56,  2.53it/s]\u001b[A\n",
            "Iteration:   3% 13/459 [00:05<02:56,  2.53it/s]\u001b[A\n",
            "Iteration:   3% 14/459 [00:05<02:55,  2.54it/s]\u001b[A\n",
            "Iteration:   3% 15/459 [00:05<02:55,  2.54it/s]\u001b[A\n",
            "Iteration:   3% 16/459 [00:06<02:54,  2.54it/s]\u001b[A\n",
            "Iteration:   4% 17/459 [00:06<02:54,  2.54it/s]\u001b[A\n",
            "Iteration:   4% 18/459 [00:07<02:54,  2.53it/s]\u001b[A\n",
            "Iteration:   4% 19/459 [00:07<02:53,  2.53it/s]\u001b[A\n",
            "Iteration:   4% 20/459 [00:07<02:52,  2.54it/s]\u001b[A\n",
            "Iteration:   5% 21/459 [00:08<02:52,  2.54it/s]\u001b[A\n",
            "Iteration:   5% 22/459 [00:08<02:51,  2.54it/s]\u001b[A\n",
            "Iteration:   5% 23/459 [00:09<02:51,  2.55it/s]\u001b[A\n",
            "Iteration:   5% 24/459 [00:09<02:50,  2.55it/s]\u001b[A\n",
            "Iteration:   5% 25/459 [00:09<02:50,  2.54it/s]\u001b[A\n",
            "Iteration:   6% 26/459 [00:10<02:49,  2.55it/s]\u001b[A\n",
            "Iteration:   6% 27/459 [00:10<02:50,  2.54it/s]\u001b[A\n",
            "Iteration:   6% 28/459 [00:11<02:49,  2.54it/s]\u001b[A\n",
            "Iteration:   6% 29/459 [00:11<02:49,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 30/459 [00:11<02:48,  2.55it/s]\u001b[A\n",
            "Iteration:   7% 31/459 [00:12<02:48,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 32/459 [00:12<02:48,  2.53it/s]\u001b[A\n",
            "Iteration:   7% 33/459 [00:13<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   7% 34/459 [00:13<02:47,  2.54it/s]\u001b[A\n",
            "Iteration:   8% 35/459 [00:13<02:46,  2.55it/s]\u001b[A\n",
            "Iteration:   8% 36/459 [00:14<02:46,  2.55it/s]\u001b[A\n",
            "Iteration:   8% 37/459 [00:14<02:46,  2.53it/s]\u001b[A\n",
            "Iteration:   8% 38/459 [00:14<02:45,  2.54it/s]\u001b[A\n",
            "Iteration:   8% 39/459 [00:15<02:46,  2.53it/s]\u001b[A\n",
            "Iteration:   9% 40/459 [00:15<02:44,  2.54it/s]\u001b[A\n",
            "Iteration:   9% 41/459 [00:16<02:45,  2.53it/s]\u001b[A\n",
            "Iteration:   9% 42/459 [00:16<02:44,  2.53it/s]\u001b[A\n",
            "Iteration:   9% 43/459 [00:16<02:44,  2.52it/s]\u001b[A\n",
            "Iteration:  10% 44/459 [00:17<02:43,  2.53it/s]\u001b[A\n",
            "Iteration:  10% 45/459 [00:17<02:43,  2.53it/s]\u001b[A\n",
            "Iteration:  10% 46/459 [00:18<02:43,  2.53it/s]\u001b[A\n",
            "Iteration:  10% 47/459 [00:18<02:42,  2.54it/s]\u001b[A\n",
            "Iteration:  10% 48/459 [00:18<02:42,  2.52it/s]\u001b[A\n",
            "Iteration:  11% 49/459 [00:19<02:42,  2.53it/s]\u001b[A\n",
            "Iteration:  11% 50/459 [00:19<02:41,  2.53it/s]\u001b[A\n",
            "Iteration:  11% 51/459 [00:20<02:41,  2.53it/s]\u001b[A\n",
            "Iteration:  11% 52/459 [00:20<02:40,  2.54it/s]\u001b[A\n",
            "Iteration:  12% 53/459 [00:20<02:39,  2.54it/s]\u001b[A\n",
            "Iteration:  12% 54/459 [00:21<02:38,  2.55it/s]\u001b[A\n",
            "Iteration:  12% 55/459 [00:21<02:40,  2.52it/s]\u001b[A\n",
            "Iteration:  12% 56/459 [00:22<02:39,  2.53it/s]\u001b[A\n",
            "Iteration:  12% 57/459 [00:22<02:38,  2.53it/s]\u001b[A\n",
            "Iteration:  13% 58/459 [00:22<02:38,  2.53it/s]\u001b[A\n",
            "Iteration:  13% 59/459 [00:23<02:37,  2.53it/s]\u001b[A\n",
            "Iteration:  13% 60/459 [00:23<02:37,  2.53it/s]\u001b[A\n",
            "Iteration:  13% 61/459 [00:24<02:37,  2.53it/s]\u001b[A\n",
            "Iteration:  14% 62/459 [00:24<02:36,  2.53it/s]\u001b[A\n",
            "Iteration:  14% 63/459 [00:24<02:36,  2.53it/s]\u001b[A\n",
            "Iteration:  14% 64/459 [00:25<02:35,  2.53it/s]\u001b[A\n",
            "Iteration:  14% 65/459 [00:25<02:35,  2.54it/s]\u001b[A\n",
            "Iteration:  14% 66/459 [00:26<02:36,  2.51it/s]\u001b[A\n",
            "Iteration:  15% 67/459 [00:26<02:35,  2.52it/s]\u001b[A\n",
            "Iteration:  15% 68/459 [00:26<02:34,  2.54it/s]\u001b[A\n",
            "Iteration:  15% 69/459 [00:27<02:33,  2.55it/s]\u001b[A\n",
            "Iteration:  15% 70/459 [00:27<02:32,  2.55it/s]\u001b[A\n",
            "Iteration:  15% 71/459 [00:28<02:32,  2.54it/s]\u001b[A\n",
            "Iteration:  16% 72/459 [00:28<02:32,  2.54it/s]\u001b[A\n",
            "Iteration:  16% 73/459 [00:28<02:33,  2.51it/s]\u001b[A\n",
            "Iteration:  16% 74/459 [00:29<02:32,  2.53it/s]\u001b[A\n",
            "Iteration:  16% 75/459 [00:29<02:31,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 76/459 [00:29<02:31,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 77/459 [00:30<02:31,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 78/459 [00:30<02:30,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 79/459 [00:31<02:29,  2.53it/s]\u001b[A\n",
            "Iteration:  17% 80/459 [00:31<02:29,  2.53it/s]\u001b[A\n",
            "Iteration:  18% 81/459 [00:31<02:29,  2.53it/s]\u001b[A\n",
            "Iteration:  18% 82/459 [00:32<02:28,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 83/459 [00:32<02:27,  2.54it/s]\u001b[A\n",
            "Iteration:  18% 84/459 [00:33<02:27,  2.53it/s]\u001b[A\n",
            "Iteration:  19% 85/459 [00:33<02:27,  2.54it/s]\u001b[A\n",
            "Iteration:  19% 86/459 [00:33<02:27,  2.53it/s]\u001b[A\n",
            "Iteration:  19% 87/459 [00:34<02:26,  2.53it/s]\u001b[A\n",
            "Iteration:  19% 88/459 [00:34<02:26,  2.53it/s]\u001b[A\n",
            "Iteration:  19% 89/459 [00:35<02:26,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 90/459 [00:35<02:26,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 91/459 [00:35<02:25,  2.53it/s]\u001b[A\n",
            "Iteration:  20% 92/459 [00:36<02:24,  2.54it/s]\u001b[A\n",
            "Iteration:  20% 93/459 [00:36<02:24,  2.54it/s]\u001b[A\n",
            "Iteration:  20% 94/459 [00:37<02:23,  2.54it/s]\u001b[A\n",
            "Iteration:  21% 95/459 [00:37<02:23,  2.54it/s]\u001b[A\n",
            "Iteration:  21% 96/459 [00:37<02:22,  2.54it/s]\u001b[A\n",
            "Iteration:  21% 97/459 [00:38<02:22,  2.54it/s]\u001b[A\n",
            "Iteration:  21% 98/459 [00:38<02:21,  2.54it/s]\u001b[A\n",
            "Iteration:  22% 99/459 [00:39<02:21,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 100/459 [00:39<02:20,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 101/459 [00:39<02:20,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 102/459 [00:40<02:20,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 103/459 [00:40<02:19,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 104/459 [00:41<02:20,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 105/459 [00:41<02:19,  2.54it/s]\u001b[A\n",
            "Iteration:  23% 106/459 [00:41<02:19,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 107/459 [00:42<02:19,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 108/459 [00:42<02:18,  2.54it/s]\u001b[A\n",
            "Iteration:  24% 109/459 [00:43<02:17,  2.54it/s]\u001b[A\n",
            "Iteration:  24% 110/459 [00:43<02:17,  2.54it/s]\u001b[A\n",
            "Iteration:  24% 111/459 [00:43<02:17,  2.53it/s]\u001b[A\n",
            "Iteration:  24% 112/459 [00:44<02:17,  2.52it/s]\u001b[A\n",
            "Iteration:  25% 113/459 [00:44<02:16,  2.53it/s]\u001b[A\n",
            "Iteration:  25% 114/459 [00:44<02:15,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 115/459 [00:45<02:16,  2.52it/s]\u001b[A\n",
            "Iteration:  25% 116/459 [00:45<02:15,  2.53it/s]\u001b[A\n",
            "Iteration:  25% 117/459 [00:46<02:15,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 118/459 [00:46<02:14,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 119/459 [00:46<02:14,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 120/459 [00:47<02:13,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 121/459 [00:47<02:13,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 122/459 [00:48<02:14,  2.50it/s]\u001b[A\n",
            "Iteration:  27% 123/459 [00:48<02:13,  2.52it/s]\u001b[A\n",
            "Iteration:  27% 124/459 [00:48<02:12,  2.52it/s]\u001b[A\n",
            "Iteration:  27% 125/459 [00:49<02:11,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 126/459 [00:49<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 127/459 [00:50<02:10,  2.55it/s]\u001b[A\n",
            "Iteration:  28% 128/459 [00:50<02:10,  2.54it/s]\u001b[A\n",
            "Iteration:  28% 129/459 [00:50<02:10,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 130/459 [00:51<02:09,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 131/459 [00:51<02:09,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 132/459 [00:52<02:08,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 133/459 [00:52<02:08,  2.54it/s]\u001b[A\n",
            "Iteration:  29% 134/459 [00:52<02:07,  2.55it/s]\u001b[A\n",
            "Iteration:  29% 135/459 [00:53<02:07,  2.55it/s]\u001b[A\n",
            "Iteration:  30% 136/459 [00:53<02:06,  2.55it/s]\u001b[A\n",
            "Iteration:  30% 137/459 [00:54<02:06,  2.55it/s]\u001b[A\n",
            "Iteration:  30% 138/459 [00:54<02:05,  2.56it/s]\u001b[A\n",
            "Iteration:  30% 139/459 [00:54<02:05,  2.56it/s]\u001b[A\n",
            "Iteration:  31% 140/459 [00:55<02:05,  2.55it/s]\u001b[A\n",
            "Iteration:  31% 141/459 [00:55<02:04,  2.55it/s]\u001b[A\n",
            "Iteration:  31% 142/459 [00:56<02:04,  2.55it/s]\u001b[A\n",
            "Iteration:  31% 143/459 [00:56<02:03,  2.55it/s]\u001b[A\n",
            "Iteration:  31% 144/459 [00:56<02:03,  2.56it/s]\u001b[A\n",
            "Iteration:  32% 145/459 [00:57<02:02,  2.56it/s]\u001b[A\n",
            "Iteration:  32% 146/459 [00:57<02:02,  2.56it/s]\u001b[A\n",
            "Iteration:  32% 147/459 [00:57<02:02,  2.55it/s]\u001b[A\n",
            "Iteration:  32% 148/459 [00:58<02:01,  2.56it/s]\u001b[A\n",
            "Iteration:  32% 149/459 [00:58<02:00,  2.56it/s]\u001b[A\n",
            "Iteration:  33% 150/459 [00:59<02:00,  2.56it/s]\u001b[A\n",
            "Iteration:  33% 151/459 [00:59<02:00,  2.57it/s]\u001b[A\n",
            "Iteration:  33% 152/459 [00:59<01:59,  2.56it/s]\u001b[A\n",
            "Iteration:  33% 153/459 [01:00<01:59,  2.56it/s]\u001b[A\n",
            "Iteration:  34% 154/459 [01:00<01:58,  2.56it/s]\u001b[A\n",
            "Iteration:  34% 155/459 [01:01<01:58,  2.57it/s]\u001b[A\n",
            "Iteration:  34% 156/459 [01:01<01:58,  2.57it/s]\u001b[A\n",
            "Iteration:  34% 157/459 [01:01<01:57,  2.57it/s]\u001b[A\n",
            "Iteration:  34% 158/459 [01:02<01:57,  2.57it/s]\u001b[A\n",
            "Iteration:  35% 159/459 [01:02<01:56,  2.57it/s]\u001b[A\n",
            "Iteration:  35% 160/459 [01:03<01:56,  2.56it/s]\u001b[A\n",
            "Iteration:  35% 161/459 [01:03<01:56,  2.55it/s]\u001b[A\n",
            "Iteration:  35% 162/459 [01:03<01:56,  2.56it/s]\u001b[A\n",
            "Iteration:  36% 163/459 [01:04<01:55,  2.56it/s]\u001b[A\n",
            "Iteration:  36% 164/459 [01:04<01:54,  2.57it/s]\u001b[A\n",
            "Iteration:  36% 165/459 [01:04<01:55,  2.56it/s]\u001b[A\n",
            "Iteration:  36% 166/459 [01:05<01:54,  2.55it/s]\u001b[A\n",
            "Iteration:  36% 167/459 [01:05<01:54,  2.56it/s]\u001b[A\n",
            "Iteration:  37% 168/459 [01:06<01:54,  2.55it/s]\u001b[A\n",
            "Iteration:  37% 169/459 [01:06<01:53,  2.55it/s]\u001b[A\n",
            "Iteration:  37% 170/459 [01:06<01:53,  2.55it/s]\u001b[A\n",
            "Iteration:  37% 171/459 [01:07<01:52,  2.56it/s]\u001b[A\n",
            "Iteration:  37% 172/459 [01:07<01:51,  2.56it/s]\u001b[A\n",
            "Iteration:  38% 173/459 [01:08<01:51,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 174/459 [01:08<01:51,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 175/459 [01:08<01:50,  2.57it/s]\u001b[A\n",
            "Iteration:  38% 176/459 [01:09<01:50,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 177/459 [01:09<01:50,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 178/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 179/459 [01:10<01:49,  2.56it/s]\u001b[A\n",
            "Iteration:  39% 180/459 [01:10<01:48,  2.57it/s]\u001b[A\n",
            "Iteration:  39% 181/459 [01:11<01:48,  2.56it/s]\u001b[A\n",
            "Iteration:  40% 182/459 [01:11<01:47,  2.57it/s]\u001b[A\n",
            "Iteration:  40% 183/459 [01:12<01:47,  2.57it/s]\u001b[A\n",
            "Iteration:  40% 184/459 [01:12<01:47,  2.57it/s]\u001b[A\n",
            "Iteration:  40% 185/459 [01:12<01:46,  2.57it/s]\u001b[A\n",
            "Iteration:  41% 186/459 [01:13<01:46,  2.56it/s]\u001b[A\n",
            "Iteration:  41% 187/459 [01:13<01:46,  2.54it/s]\u001b[A\n",
            "Iteration:  41% 188/459 [01:13<01:46,  2.55it/s]\u001b[A\n",
            "Iteration:  41% 189/459 [01:14<01:45,  2.56it/s]\u001b[A\n",
            "Iteration:  41% 190/459 [01:14<01:45,  2.55it/s]\u001b[A\n",
            "Iteration:  42% 191/459 [01:15<01:44,  2.56it/s]\u001b[A\n",
            "Iteration:  42% 192/459 [01:15<01:44,  2.57it/s]\u001b[A\n",
            "Iteration:  42% 193/459 [01:15<01:44,  2.55it/s]\u001b[A\n",
            "Iteration:  42% 194/459 [01:16<01:43,  2.56it/s]\u001b[A\n",
            "Iteration:  42% 195/459 [01:16<01:43,  2.55it/s]\u001b[A\n",
            "Iteration:  43% 196/459 [01:17<01:42,  2.56it/s]\u001b[A\n",
            "Iteration:  43% 197/459 [01:17<01:42,  2.56it/s]\u001b[A\n",
            "Iteration:  43% 198/459 [01:17<01:42,  2.56it/s]\u001b[A\n",
            "Iteration:  43% 199/459 [01:18<01:41,  2.56it/s]\u001b[A\n",
            "Iteration:  44% 200/459 [01:18<01:42,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 201/459 [01:19<01:40,  2.56it/s]\u001b[A\n",
            "Iteration:  44% 202/459 [01:19<01:40,  2.55it/s]\u001b[A\n",
            "Iteration:  44% 203/459 [01:19<01:40,  2.54it/s]\u001b[A\n",
            "Iteration:  44% 204/459 [01:20<01:40,  2.54it/s]\u001b[A\n",
            "Iteration:  45% 205/459 [01:20<01:39,  2.55it/s]\u001b[A\n",
            "Iteration:  45% 206/459 [01:21<01:39,  2.53it/s]\u001b[A\n",
            "Iteration:  45% 207/459 [01:21<01:39,  2.54it/s]\u001b[A\n",
            "Iteration:  45% 208/459 [01:21<01:39,  2.53it/s]\u001b[A\n",
            "Iteration:  46% 209/459 [01:22<01:38,  2.53it/s]\u001b[A\n",
            "Iteration:  46% 210/459 [01:22<01:38,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 211/459 [01:22<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 212/459 [01:23<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  46% 213/459 [01:23<01:37,  2.53it/s]\u001b[A\n",
            "Iteration:  47% 214/459 [01:24<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  47% 215/459 [01:24<01:35,  2.55it/s]\u001b[A\n",
            "Iteration:  47% 216/459 [01:24<01:35,  2.55it/s]\u001b[A\n",
            "Iteration:  47% 217/459 [01:25<01:35,  2.55it/s]\u001b[A\n",
            "Iteration:  47% 218/459 [01:25<01:34,  2.55it/s]\u001b[A\n",
            "Iteration:  48% 219/459 [01:26<01:35,  2.51it/s]\u001b[A\n",
            "Iteration:  48% 220/459 [01:26<01:34,  2.53it/s]\u001b[A\n",
            "Iteration:  48% 221/459 [01:26<01:34,  2.52it/s]\u001b[A\n",
            "Iteration:  48% 222/459 [01:27<01:33,  2.53it/s]\u001b[A\n",
            "Iteration:  49% 223/459 [01:27<01:33,  2.53it/s]\u001b[A\n",
            "Iteration:  49% 224/459 [01:28<01:32,  2.53it/s]\u001b[A\n",
            "Iteration:  49% 225/459 [01:28<01:32,  2.54it/s]\u001b[A\n",
            "Iteration:  49% 226/459 [01:28<01:32,  2.52it/s]\u001b[A\n",
            "Iteration:  49% 227/459 [01:29<01:31,  2.53it/s]\u001b[A\n",
            "Iteration:  50% 228/459 [01:29<01:31,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 229/459 [01:30<01:30,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 230/459 [01:30<01:29,  2.54it/s]\u001b[A\n",
            "Iteration:  50% 231/459 [01:30<01:29,  2.55it/s]\u001b[A\n",
            "Iteration:  51% 232/459 [01:31<01:29,  2.54it/s]\u001b[A\n",
            "Iteration:  51% 233/459 [01:31<01:29,  2.54it/s]\u001b[A\n",
            "Iteration:  51% 234/459 [01:32<01:28,  2.54it/s]\u001b[A\n",
            "Iteration:  51% 235/459 [01:32<01:28,  2.53it/s]\u001b[A\n",
            "Iteration:  51% 236/459 [01:32<01:28,  2.53it/s]\u001b[A\n",
            "Iteration:  52% 237/459 [01:33<01:27,  2.54it/s]\u001b[A\n",
            "Iteration:  52% 238/459 [01:33<01:27,  2.54it/s]\u001b[A\n",
            "Iteration:  52% 239/459 [01:34<01:26,  2.54it/s]\u001b[A\n",
            "Iteration:  52% 240/459 [01:34<01:26,  2.53it/s]\u001b[A\n",
            "Iteration:  53% 241/459 [01:34<01:25,  2.55it/s]\u001b[A\n",
            "Iteration:  53% 242/459 [01:35<01:25,  2.55it/s]\u001b[A\n",
            "Iteration:  53% 243/459 [01:35<01:24,  2.55it/s]\u001b[A\n",
            "Iteration:  53% 244/459 [01:35<01:24,  2.54it/s]\u001b[A\n",
            "Iteration:  53% 245/459 [01:36<01:24,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 246/459 [01:36<01:23,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 247/459 [01:37<01:23,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 248/459 [01:37<01:23,  2.54it/s]\u001b[A\n",
            "Iteration:  54% 249/459 [01:37<01:23,  2.52it/s]\u001b[A\n",
            "Iteration:  54% 250/459 [01:38<01:22,  2.52it/s]\u001b[A\n",
            "Iteration:  55% 251/459 [01:38<01:21,  2.54it/s]\u001b[A\n",
            "Iteration:  55% 252/459 [01:39<01:21,  2.53it/s]\u001b[A\n",
            "Iteration:  55% 253/459 [01:39<01:21,  2.53it/s]\u001b[A\n",
            "Iteration:  55% 254/459 [01:39<01:20,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 255/459 [01:40<01:20,  2.55it/s]\u001b[A\n",
            "Iteration:  56% 256/459 [01:40<01:20,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 257/459 [01:41<01:19,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 258/459 [01:41<01:19,  2.54it/s]\u001b[A\n",
            "Iteration:  56% 259/459 [01:41<01:18,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 260/459 [01:42<01:18,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 261/459 [01:42<01:17,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 262/459 [01:43<01:17,  2.54it/s]\u001b[A\n",
            "Iteration:  57% 263/459 [01:43<01:16,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 264/459 [01:43<01:16,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 265/459 [01:44<01:16,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 266/459 [01:44<01:15,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 267/459 [01:45<01:15,  2.55it/s]\u001b[A\n",
            "Iteration:  58% 268/459 [01:45<01:15,  2.53it/s]\u001b[A\n",
            "Iteration:  59% 269/459 [01:45<01:14,  2.55it/s]\u001b[A\n",
            "Iteration:  59% 270/459 [01:46<01:14,  2.54it/s]\u001b[A\n",
            "Iteration:  59% 271/459 [01:46<01:13,  2.54it/s]\u001b[A\n",
            "Iteration:  59% 272/459 [01:47<01:13,  2.54it/s]\u001b[A\n",
            "Iteration:  59% 273/459 [01:47<01:13,  2.54it/s]\u001b[A\n",
            "Iteration:  60% 274/459 [01:47<01:12,  2.54it/s]\u001b[A\n",
            "Iteration:  60% 275/459 [01:48<01:12,  2.54it/s]\u001b[A\n",
            "Iteration:  60% 276/459 [01:48<01:12,  2.54it/s]\u001b[A\n",
            "Iteration:  60% 277/459 [01:48<01:11,  2.54it/s]\u001b[A\n",
            "Iteration:  61% 278/459 [01:49<01:11,  2.53it/s]\u001b[A\n",
            "Iteration:  61% 279/459 [01:49<01:10,  2.54it/s]\u001b[A\n",
            "Iteration:  61% 280/459 [01:50<01:10,  2.53it/s]\u001b[A\n",
            "Iteration:  61% 281/459 [01:50<01:10,  2.53it/s]\u001b[A\n",
            "Iteration:  61% 282/459 [01:50<01:09,  2.54it/s]\u001b[A\n",
            "Iteration:  62% 283/459 [01:51<01:09,  2.53it/s]\u001b[A\n",
            "Iteration:  62% 284/459 [01:51<01:09,  2.54it/s]\u001b[A\n",
            "Iteration:  62% 285/459 [01:52<01:08,  2.54it/s]\u001b[A\n",
            "Iteration:  62% 286/459 [01:52<01:08,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 287/459 [01:52<01:07,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 288/459 [01:53<01:07,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 289/459 [01:53<01:07,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 290/459 [01:54<01:06,  2.53it/s]\u001b[A\n",
            "Iteration:  63% 291/459 [01:54<01:06,  2.54it/s]\u001b[A\n",
            "Iteration:  64% 292/459 [01:54<01:05,  2.53it/s]\u001b[A\n",
            "Iteration:  64% 293/459 [01:55<01:05,  2.55it/s]\u001b[A\n",
            "Iteration:  64% 294/459 [01:55<01:04,  2.55it/s]\u001b[A\n",
            "Iteration:  64% 295/459 [01:56<01:03,  2.56it/s]\u001b[A\n",
            "Iteration:  64% 296/459 [01:56<01:03,  2.55it/s]\u001b[A\n",
            "Iteration:  65% 297/459 [01:56<01:03,  2.55it/s]\u001b[A\n",
            "Iteration:  65% 298/459 [01:57<01:03,  2.54it/s]\u001b[A\n",
            "Iteration:  65% 299/459 [01:57<01:02,  2.55it/s]\u001b[A\n",
            "Iteration:  65% 300/459 [01:58<01:02,  2.55it/s]\u001b[A\n",
            "Iteration:  66% 301/459 [01:58<01:02,  2.54it/s]\u001b[A\n",
            "Iteration:  66% 302/459 [01:58<01:01,  2.54it/s]\u001b[A\n",
            "Iteration:  66% 303/459 [01:59<01:01,  2.52it/s]\u001b[A\n",
            "Iteration:  66% 304/459 [01:59<01:01,  2.54it/s]\u001b[A\n",
            "Iteration:  66% 305/459 [02:00<01:00,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 306/459 [02:00<01:00,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 307/459 [02:00<00:59,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 308/459 [02:01<00:59,  2.54it/s]\u001b[A\n",
            "Iteration:  67% 309/459 [02:01<00:59,  2.54it/s]\u001b[A\n",
            "Iteration:  68% 310/459 [02:01<00:58,  2.54it/s]\u001b[A\n",
            "Iteration:  68% 311/459 [02:02<00:58,  2.52it/s]\u001b[A\n",
            "Iteration:  68% 312/459 [02:02<00:58,  2.53it/s]\u001b[A\n",
            "Iteration:  68% 313/459 [02:03<00:57,  2.53it/s]\u001b[A\n",
            "Iteration:  68% 314/459 [02:03<00:57,  2.53it/s]\u001b[A\n",
            "Iteration:  69% 315/459 [02:03<00:56,  2.53it/s]\u001b[A\n",
            "Iteration:  69% 316/459 [02:04<00:56,  2.51it/s]\u001b[A\n",
            "Iteration:  69% 317/459 [02:04<00:56,  2.52it/s]\u001b[A\n",
            "Iteration:  69% 318/459 [02:05<00:55,  2.52it/s]\u001b[A\n",
            "Iteration:  69% 319/459 [02:05<00:55,  2.53it/s]\u001b[A\n",
            "Iteration:  70% 320/459 [02:05<00:54,  2.53it/s]\u001b[A\n",
            "Iteration:  70% 321/459 [02:06<00:54,  2.54it/s]\u001b[A\n",
            "Iteration:  70% 322/459 [02:06<00:53,  2.54it/s]\u001b[A\n",
            "Iteration:  70% 323/459 [02:07<00:53,  2.54it/s]\u001b[A\n",
            "Iteration:  71% 324/459 [02:07<00:53,  2.54it/s]\u001b[A\n",
            "Iteration:  71% 325/459 [02:07<00:52,  2.55it/s]\u001b[A\n",
            "Iteration:  71% 326/459 [02:08<00:52,  2.56it/s]\u001b[A\n",
            "Iteration:  71% 327/459 [02:08<00:51,  2.56it/s]\u001b[A\n",
            "Iteration:  71% 328/459 [02:09<00:51,  2.56it/s]\u001b[A\n",
            "Iteration:  72% 329/459 [02:09<00:50,  2.55it/s]\u001b[A\n",
            "Iteration:  72% 330/459 [02:09<00:50,  2.56it/s]\u001b[A\n",
            "Iteration:  72% 331/459 [02:10<00:50,  2.56it/s]\u001b[A\n",
            "Iteration:  72% 332/459 [02:10<00:49,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 333/459 [02:11<00:49,  2.54it/s]\u001b[A\n",
            "Iteration:  73% 334/459 [02:11<00:48,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 335/459 [02:11<00:48,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 336/459 [02:12<00:48,  2.56it/s]\u001b[A\n",
            "Iteration:  73% 337/459 [02:12<00:47,  2.56it/s]\u001b[A\n",
            "Iteration:  74% 338/459 [02:12<00:47,  2.55it/s]\u001b[A\n",
            "Iteration:  74% 339/459 [02:13<00:47,  2.55it/s]\u001b[A\n",
            "Iteration:  74% 340/459 [02:13<00:46,  2.55it/s]\u001b[A\n",
            "Iteration:  74% 341/459 [02:14<00:46,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 342/459 [02:14<00:45,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 343/459 [02:14<00:45,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 344/459 [02:15<00:45,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 345/459 [02:15<00:44,  2.55it/s]\u001b[A\n",
            "Iteration:  75% 346/459 [02:16<00:44,  2.54it/s]\u001b[A\n",
            "Iteration:  76% 347/459 [02:16<00:44,  2.54it/s]\u001b[A\n",
            "Iteration:  76% 348/459 [02:16<00:43,  2.54it/s]\u001b[A\n",
            "Iteration:  76% 349/459 [02:17<00:43,  2.53it/s]\u001b[A\n",
            "Iteration:  76% 350/459 [02:17<00:43,  2.52it/s]\u001b[A\n",
            "Iteration:  76% 351/459 [02:18<00:42,  2.54it/s]\u001b[A\n",
            "Iteration:  77% 352/459 [02:18<00:42,  2.53it/s]\u001b[A\n",
            "Iteration:  77% 353/459 [02:18<00:41,  2.53it/s]\u001b[A\n",
            "Iteration:  77% 354/459 [02:19<00:41,  2.53it/s]\u001b[A\n",
            "Iteration:  77% 355/459 [02:19<00:41,  2.51it/s]\u001b[A\n",
            "Iteration:  78% 356/459 [02:20<00:40,  2.54it/s]\u001b[A\n",
            "Iteration:  78% 357/459 [02:20<00:40,  2.53it/s]\u001b[A\n",
            "Iteration:  78% 358/459 [02:20<00:39,  2.55it/s]\u001b[A\n",
            "Iteration:  78% 359/459 [02:21<00:39,  2.54it/s]\u001b[A\n",
            "Iteration:  78% 360/459 [02:21<00:38,  2.54it/s]\u001b[A\n",
            "Iteration:  79% 361/459 [02:22<00:38,  2.54it/s]\u001b[A\n",
            "Iteration:  79% 362/459 [02:22<00:38,  2.53it/s]\u001b[A\n",
            "Iteration:  79% 363/459 [02:22<00:37,  2.54it/s]\u001b[A\n",
            "Iteration:  79% 364/459 [02:23<00:37,  2.54it/s]\u001b[A\n",
            "Iteration:  80% 365/459 [02:23<00:37,  2.54it/s]\u001b[A\n",
            "Iteration:  80% 366/459 [02:24<00:36,  2.54it/s]\u001b[A\n",
            "Iteration:  80% 367/459 [02:24<00:36,  2.54it/s]\u001b[A\n",
            "Iteration:  80% 368/459 [02:24<00:36,  2.53it/s]\u001b[A\n",
            "Iteration:  80% 369/459 [02:25<00:35,  2.54it/s]\u001b[A\n",
            "Iteration:  81% 370/459 [02:25<00:34,  2.56it/s]\u001b[A\n",
            "Iteration:  81% 371/459 [02:25<00:34,  2.56it/s]\u001b[A\n",
            "Iteration:  81% 372/459 [02:26<00:34,  2.56it/s]\u001b[A\n",
            "Iteration:  81% 373/459 [02:26<00:33,  2.55it/s]\u001b[A\n",
            "Iteration:  81% 374/459 [02:27<00:33,  2.55it/s]\u001b[A\n",
            "Iteration:  82% 375/459 [02:27<00:32,  2.55it/s]\u001b[A\n",
            "Iteration:  82% 376/459 [02:27<00:32,  2.54it/s]\u001b[A\n",
            "Iteration:  82% 377/459 [02:28<00:32,  2.54it/s]\u001b[A\n",
            "Iteration:  82% 378/459 [02:28<00:31,  2.55it/s]\u001b[A\n",
            "Iteration:  83% 379/459 [02:29<00:31,  2.55it/s]\u001b[A\n",
            "Iteration:  83% 380/459 [02:29<00:31,  2.55it/s]\u001b[A\n",
            "Iteration:  83% 381/459 [02:29<00:30,  2.56it/s]\u001b[A\n",
            "Iteration:  83% 382/459 [02:30<00:30,  2.54it/s]\u001b[A\n",
            "Iteration:  83% 383/459 [02:30<00:29,  2.55it/s]\u001b[A\n",
            "Iteration:  84% 384/459 [02:31<00:29,  2.53it/s]\u001b[A\n",
            "Iteration:  84% 385/459 [02:31<00:29,  2.53it/s]\u001b[A\n",
            "Iteration:  84% 386/459 [02:31<00:28,  2.52it/s]\u001b[A\n",
            "Iteration:  84% 387/459 [02:32<00:28,  2.53it/s]\u001b[A\n",
            "Iteration:  85% 388/459 [02:32<00:28,  2.53it/s]\u001b[A\n",
            "Iteration:  85% 389/459 [02:33<00:27,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 390/459 [02:33<00:27,  2.53it/s]\u001b[A\n",
            "Iteration:  85% 391/459 [02:33<00:26,  2.54it/s]\u001b[A\n",
            "Iteration:  85% 392/459 [02:34<00:26,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 393/459 [02:34<00:25,  2.55it/s]\u001b[A\n",
            "Iteration:  86% 394/459 [02:35<00:25,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 395/459 [02:35<00:25,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 396/459 [02:35<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  86% 397/459 [02:36<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 398/459 [02:36<00:24,  2.54it/s]\u001b[A\n",
            "Iteration:  87% 399/459 [02:37<00:23,  2.55it/s]\u001b[A\n",
            "Iteration:  87% 400/459 [02:37<00:23,  2.55it/s]\u001b[A\n",
            "Iteration:  87% 401/459 [02:37<00:22,  2.54it/s]\u001b[A\n",
            "Iteration:  88% 402/459 [02:38<00:22,  2.54it/s]\u001b[A\n",
            "Iteration:  88% 403/459 [02:38<00:21,  2.55it/s]\u001b[A\n",
            "Iteration:  88% 404/459 [02:38<00:21,  2.55it/s]\u001b[A\n",
            "Iteration:  88% 405/459 [02:39<00:21,  2.55it/s]\u001b[A\n",
            "Iteration:  88% 406/459 [02:39<00:20,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 407/459 [02:40<00:20,  2.54it/s]\u001b[A\n",
            "Iteration:  89% 408/459 [02:40<00:20,  2.53it/s]\u001b[A\n",
            "Iteration:  89% 409/459 [02:40<00:19,  2.53it/s]\u001b[A\n",
            "Iteration:  89% 410/459 [02:41<00:19,  2.53it/s]\u001b[A\n",
            "Iteration:  90% 411/459 [02:41<00:18,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 412/459 [02:42<00:18,  2.53it/s]\u001b[A\n",
            "Iteration:  90% 413/459 [02:42<00:18,  2.53it/s]\u001b[A\n",
            "Iteration:  90% 414/459 [02:42<00:17,  2.54it/s]\u001b[A\n",
            "Iteration:  90% 415/459 [02:43<00:17,  2.54it/s]\u001b[A\n",
            "Iteration:  91% 416/459 [02:43<00:16,  2.54it/s]\u001b[A\n",
            "Iteration:  91% 417/459 [02:44<00:16,  2.52it/s]\u001b[A\n",
            "Iteration:  91% 418/459 [02:44<00:16,  2.53it/s]\u001b[A\n",
            "Iteration:  91% 419/459 [02:44<00:15,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 420/459 [02:45<00:15,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 421/459 [02:45<00:15,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 422/459 [02:46<00:14,  2.53it/s]\u001b[A\n",
            "Iteration:  92% 423/459 [02:46<00:14,  2.54it/s]\u001b[A\n",
            "Iteration:  92% 424/459 [02:46<00:13,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 425/459 [02:47<00:13,  2.53it/s]\u001b[A\n",
            "Iteration:  93% 426/459 [02:47<00:13,  2.53it/s]\u001b[A\n",
            "Iteration:  93% 427/459 [02:48<00:12,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 428/459 [02:48<00:12,  2.54it/s]\u001b[A\n",
            "Iteration:  93% 429/459 [02:48<00:11,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 430/459 [02:49<00:11,  2.55it/s]\u001b[A\n",
            "Iteration:  94% 431/459 [02:49<00:11,  2.54it/s]\u001b[A\n",
            "Iteration:  94% 432/459 [02:50<00:10,  2.55it/s]\u001b[A\n",
            "Iteration:  94% 433/459 [02:50<00:10,  2.55it/s]\u001b[A\n",
            "Iteration:  95% 434/459 [02:50<00:09,  2.55it/s]\u001b[A\n",
            "Iteration:  95% 435/459 [02:51<00:09,  2.55it/s]\u001b[A\n",
            "Iteration:  95% 436/459 [02:51<00:09,  2.54it/s]\u001b[A\n",
            "Iteration:  95% 437/459 [02:51<00:08,  2.53it/s]\u001b[A\n",
            "Iteration:  95% 438/459 [02:52<00:08,  2.53it/s]\u001b[A\n",
            "Iteration:  96% 439/459 [02:52<00:07,  2.52it/s]\u001b[A\n",
            "Iteration:  96% 440/459 [02:53<00:07,  2.53it/s]\u001b[A\n",
            "Iteration:  96% 441/459 [02:53<00:07,  2.52it/s]\u001b[A\n",
            "Iteration:  96% 442/459 [02:53<00:06,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 443/459 [02:54<00:06,  2.52it/s]\u001b[A\n",
            "Iteration:  97% 444/459 [02:54<00:05,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 445/459 [02:55<00:05,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 446/459 [02:55<00:05,  2.53it/s]\u001b[A\n",
            "Iteration:  97% 447/459 [02:55<00:04,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 448/459 [02:56<00:04,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 449/459 [02:56<00:03,  2.55it/s]\u001b[A\n",
            "Iteration:  98% 450/459 [02:57<00:03,  2.54it/s]\u001b[A\n",
            "Iteration:  98% 451/459 [02:57<00:03,  2.53it/s]\u001b[A\n",
            "Iteration:  98% 452/459 [02:57<00:02,  2.54it/s]\u001b[A\n",
            "Iteration:  99% 453/459 [02:58<00:02,  2.51it/s]\u001b[A\n",
            "Iteration:  99% 454/459 [02:58<00:01,  2.52it/s]\u001b[A\n",
            "Iteration:  99% 455/459 [02:59<00:01,  2.53it/s]\u001b[A\n",
            "Iteration:  99% 456/459 [02:59<00:01,  2.52it/s]\u001b[A\n",
            "Iteration: 100% 457/459 [02:59<00:00,  2.53it/s]\u001b[A\n",
            "Iteration: 100% 458/459 [03:00<00:00,  2.53it/s]\u001b[A\n",
            "Iteration: 100% 459/459 [03:00<00:00,  2.54it/s]\n",
            "Epoch: 100% 3/3 [09:00<00:00, 180.16s/it]\n",
            "11/29/2021 03:30:24 - INFO - __main__ -   Saving model checkpoint to ./saved_models/bert-base/MRPC/two_stage\n",
            "11/29/2021 03:30:24 - INFO - transformers.configuration_utils -   Configuration saved in ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:30:29 - INFO - transformers.modeling_utils -   Model weights saved in ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "11/29/2021 03:30:31 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:30:31 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:30:31 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "11/29/2021 03:30:34 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:30:34 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:30:34 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:30:34 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:30:34 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:30:35 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:30:35 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:30:35 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:30:35 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:30:35 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:30:35 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:30:35 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:30:35 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:30:35 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "-1\n",
            "11/29/2021 03:30:40 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:30:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:30:41 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc\n",
            "11/29/2021 03:30:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:30:41 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:30:41 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 29.68it/s]\n",
            "Eval time: 13.746034383773804\n",
            "11/29/2021 03:30:54 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:30:54 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:30:54 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:30:54 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/train_highway.sh {PATH_TO_DATA} {MODEL_TYPE} {MODEL_SIZE} {DATASET}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0mxBsXxadUd"
      },
      "source": [
        "### This is for evaluating each exit layer for fine-tuned DeeBERT models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUw9rF-rDxf_",
        "outputId": "d11eec40-50b3-48cc-f323-c69b66c2adb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-11-29 03:31:04.218416: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:31:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:31:04 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:31:04 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:31:04 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:31:04 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:31:04 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:31:04 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:31:04 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:31:04 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "-1\n",
            "11/29/2021 03:31:08 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=-1, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=True, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:31:08 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:31:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:31:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:31:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:31:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:31:08 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:31:08 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:31:08 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:31:08 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "-1\n",
            "11/29/2021 03:31:12 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:12 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:13 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:31:13 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:31:13 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:31:13 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.23it/s]\n",
            "Eval time: 13.498697519302368\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:31:26 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:31:26 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:31:26 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:31:26 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "11/29/2021 03:31:26 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:31:26 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:26 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:27 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:31:27 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:31:27 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:31:27 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.56it/s]\n",
            "Eval time: 13.349217653274536\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:31:40 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:31:40 - INFO - __main__ -     acc = 0.6838235294117647\n",
            "11/29/2021 03:31:40 - INFO - __main__ -     acc_and_f1 = 0.7480253018237863\n",
            "11/29/2021 03:31:40 - INFO - __main__ -     f1 = 0.8122270742358079\n",
            "11/29/2021 03:31:40 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:31:40 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:40 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:41 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:31:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:31:41 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:31:41 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.50it/s]\n",
            "Eval time: 13.375584363937378\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:31:54 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:31:54 - INFO - __main__ -     acc = 0.6838235294117647\n",
            "11/29/2021 03:31:54 - INFO - __main__ -     acc_and_f1 = 0.7480253018237863\n",
            "11/29/2021 03:31:54 - INFO - __main__ -     f1 = 0.8122270742358079\n",
            "11/29/2021 03:31:54 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:31:54 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:31:54 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:31:55 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:31:55 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:31:55 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:31:55 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.70it/s]\n",
            "Eval time: 13.290260314941406\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:32:08 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:32:08 - INFO - __main__ -     acc = 0.6838235294117647\n",
            "11/29/2021 03:32:08 - INFO - __main__ -     acc_and_f1 = 0.7480253018237863\n",
            "11/29/2021 03:32:08 - INFO - __main__ -     f1 = 0.8122270742358079\n",
            "11/29/2021 03:32:08 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:32:08 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:08 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:32:09 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:32:09 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:32:09 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.66it/s]\n",
            "Eval time: 13.308037281036377\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:32:22 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:32:22 - INFO - __main__ -     acc = 0.6985294117647058\n",
            "11/29/2021 03:32:22 - INFO - __main__ -     acc_and_f1 = 0.7586903318028242\n",
            "11/29/2021 03:32:22 - INFO - __main__ -     f1 = 0.8188512518409425\n",
            "11/29/2021 03:32:22 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:32:22 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:22 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:22 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:32:22 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:32:22 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:32:22 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.63it/s]\n",
            "Eval time: 13.321304082870483\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:32:36 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:32:36 - INFO - __main__ -     acc = 0.7450980392156863\n",
            "11/29/2021 03:32:36 - INFO - __main__ -     acc_and_f1 = 0.7923021060275963\n",
            "11/29/2021 03:32:36 - INFO - __main__ -     f1 = 0.8395061728395062\n",
            "11/29/2021 03:32:36 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:32:36 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:36 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:36 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:32:36 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:32:36 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:32:36 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.50it/s]\n",
            "Eval time: 13.379031658172607\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:32:50 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:32:50 - INFO - __main__ -     acc = 0.7401960784313726\n",
            "11/29/2021 03:32:50 - INFO - __main__ -     acc_and_f1 = 0.7843375214163335\n",
            "11/29/2021 03:32:50 - INFO - __main__ -     f1 = 0.8284789644012944\n",
            "11/29/2021 03:32:50 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:32:50 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:32:50 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:32:50 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:32:50 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:32:50 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:32:50 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.53it/s]\n",
            "Eval time: 13.365188598632812\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:33:03 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:33:03 - INFO - __main__ -     acc = 0.7549019607843137\n",
            "11/29/2021 03:33:03 - INFO - __main__ -     acc_and_f1 = 0.7965448315248429\n",
            "11/29/2021 03:33:03 - INFO - __main__ -     f1 = 0.8381877022653722\n",
            "11/29/2021 03:33:03 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:33:03 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:03 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:04 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:33:04 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:33:04 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:33:04 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.45it/s]\n",
            "Eval time: 13.400452375411987\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:33:17 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:33:17 - INFO - __main__ -     acc = 0.7524509803921569\n",
            "11/29/2021 03:33:17 - INFO - __main__ -     acc_and_f1 = 0.7951661001479244\n",
            "11/29/2021 03:33:17 - INFO - __main__ -     f1 = 0.8378812199036919\n",
            "11/29/2021 03:33:17 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:33:17 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:17 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:18 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:33:18 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:33:18 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:33:18 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.56it/s]\n",
            "Eval time: 13.353410959243774\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:33:31 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:33:31 - INFO - __main__ -     acc = 0.7573529411764706\n",
            "11/29/2021 03:33:31 - INFO - __main__ -     acc_and_f1 = 0.7987087807659412\n",
            "11/29/2021 03:33:31 - INFO - __main__ -     f1 = 0.840064620355412\n",
            "11/29/2021 03:33:31 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:33:31 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:32 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:33:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:33:32 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:33:32 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.47it/s]\n",
            "Eval time: 13.390572786331177\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:33:45 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:33:45 - INFO - __main__ -     acc = 0.7843137254901961\n",
            "11/29/2021 03:33:45 - INFO - __main__ -     acc_and_f1 = 0.8204956249600818\n",
            "11/29/2021 03:33:45 - INFO - __main__ -     f1 = 0.8566775244299675\n",
            "11/29/2021 03:33:45 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:33:45 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:45 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:46 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:33:46 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:33:46 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:33:46 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.52it/s]\n",
            "Eval time: 13.369138956069946\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:33:59 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:33:59 - INFO - __main__ -     acc = 0.8259803921568627\n",
            "11/29/2021 03:33:59 - INFO - __main__ -     acc_and_f1 = 0.8535262094787663\n",
            "11/29/2021 03:33:59 - INFO - __main__ -     f1 = 0.88107202680067\n",
            "11/29/2021 03:33:59 - INFO - __main__ -   \n",
            "\n",
            "11/29/2021 03:33:59 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:33:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:34:00 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:34:00 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:34:00 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:34:00 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.54it/s]\n",
            "Eval time: 13.359055042266846\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:34:13 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:34:13 - INFO - __main__ -     acc = 0.8504901960784313\n",
            "11/29/2021 03:34:13 - INFO - __main__ -     acc_and_f1 = 0.8682357522448232\n",
            "11/29/2021 03:34:13 - INFO - __main__ -     f1 = 0.8859813084112149\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/eval_highway.sh {PATH_TO_DATA} {MODEL_TYPE} {MODEL_SIZE} {DATASET}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxrab0c7adUf"
      },
      "source": [
        "### This is for evaluating fine-tuned DeeBERT models, given a number of different early exit entropy thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPTUPCUlDqiF",
        "outputId": "89fd6141-10d7-41ef-8aee-c03e8fa068b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "2021-11-29 03:34:22.330725: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:34:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:34:22 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:34:22 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:34:22 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:34:22 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:34:22 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:34:22 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:34:22 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:34:22 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.0\n",
            "11/29/2021 03:34:26 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.0, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:34:26 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:34:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:34:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:34:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:34:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:34:26 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:34:26 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:34:26 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:34:26 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.0\n",
            "11/29/2021 03:34:30 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:34:30 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:34:31 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:34:31 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:34:31 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:34:31 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.32it/s]\n",
            "Eval time: 13.458903312683105\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:34:44 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:34:44 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:34:44 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:34:44 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.001\n",
            "2021-11-29 03:34:53.122383: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:34:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:34:53 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:34:53 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:34:53 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:34:53 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:34:53 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:34:53 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:34:53 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:34:53 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.001\n",
            "11/29/2021 03:34:57 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.001, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:34:57 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:34:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:34:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:34:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:34:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:34:57 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:34:57 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:34:57 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:34:57 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.001\n",
            "11/29/2021 03:35:01 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:01 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:35:01 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:35:01 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:35:01 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.66it/s]\n",
            "Eval time: 13.309908628463745\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:35:15 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:35:15 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:35:15 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:35:15 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.005\n",
            "2021-11-29 03:35:23.478892: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:35:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:35:23 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:35:23 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:35:23 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:35:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:35:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:35:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:35:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:35:23 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.005\n",
            "11/29/2021 03:35:27 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.005, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:35:27 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:35:27 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:35:27 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:35:27 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:35:27 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:35:27 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:35:27 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:35:27 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:35:27 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.005\n",
            "11/29/2021 03:35:31 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:35:31 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:35:32 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:35:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:35:32 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:35:32 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.02it/s]\n",
            "Eval time: 13.593218088150024\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:35:46 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:35:46 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:35:46 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:35:46 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.01\n",
            "2021-11-29 03:35:54.328299: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:35:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:35:54 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:35:54 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:35:54 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:35:54 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:35:54 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:35:54 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:35:54 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:35:54 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.01\n",
            "11/29/2021 03:35:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.01, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:35:58 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:35:58 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:35:58 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:35:58 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:35:58 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:35:58 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:35:58 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:35:58 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:35:58 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.01\n",
            "11/29/2021 03:36:02 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:03 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:36:03 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:36:03 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:36:03 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.44it/s]\n",
            "Eval time: 13.405313730239868\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            "Expected saving 1.0\n",
            "11/29/2021 03:36:16 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:36:16 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:36:16 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:36:16 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.05\n",
            "2021-11-29 03:36:25.018185: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:36:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:36:25 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:36:25 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:36:25 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:36:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:36:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:36:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:36:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:36:25 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.05\n",
            "11/29/2021 03:36:29 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.05, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:36:29 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:36:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:36:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:36:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:36:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:36:29 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:36:29 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:36:29 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:36:29 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.05\n",
            "11/29/2021 03:36:33 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:36:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:36:34 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:36:34 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:36:34 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:36:34 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:13<00:00, 30.10it/s]\n",
            "Eval time: 13.55452013015747\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 41, 12: 367}\n",
            "Expected saving 0.9916258169934641\n",
            "11/29/2021 03:36:47 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:36:47 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:36:47 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:36:47 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.1\n",
            "2021-11-29 03:36:55.938685: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:36:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:36:55 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:36:55 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:36:55 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:36:55 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:36:55 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:36:55 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:36:55 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:36:56 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.1\n",
            "11/29/2021 03:37:00 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.1, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:37:00 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:37:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:37:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:37:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:37:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:37:00 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:37:00 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:37:00 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:37:00 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.1\n",
            "11/29/2021 03:37:04 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:04 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:37:04 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:37:04 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:37:04 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:12<00:00, 31.53it/s]\n",
            "Eval time: 12.941124200820923\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 4, 7: 1, 8: 1, 9: 0, 10: 21, 11: 110, 12: 271}\n",
            "Expected saving 0.9622140522875817\n",
            "11/29/2021 03:37:17 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:37:17 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:37:17 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:37:17 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.15\n",
            "2021-11-29 03:37:26.167624: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:37:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:37:26 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:37:26 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:37:26 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:37:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:37:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:37:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:37:26 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:37:26 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.15\n",
            "11/29/2021 03:37:30 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.15, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:37:30 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:37:30 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:37:30 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:37:30 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:37:30 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:37:30 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:37:30 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:37:30 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:37:30 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.15\n",
            "11/29/2021 03:37:34 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:37:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:37:35 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:37:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:37:35 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:37:35 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:12<00:00, 32.24it/s]\n",
            "Eval time: 12.6563560962677\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 15, 7: 14, 8: 2, 9: 3, 10: 60, 11: 112, 12: 202}\n",
            "Expected saving 0.9164624183006536\n",
            "11/29/2021 03:37:47 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:37:47 - INFO - __main__ -     acc = 0.8602941176470589\n",
            "11/29/2021 03:37:47 - INFO - __main__ -     acc_and_f1 = 0.8805818414322251\n",
            "11/29/2021 03:37:47 - INFO - __main__ -     f1 = 0.9008695652173914\n",
            "Result: 0.9008695652173914\n",
            "0.2\n",
            "2021-11-29 03:37:56.111301: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:37:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:37:56 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:37:56 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:37:56 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:37:56 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:37:56 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:37:56 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:37:56 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:37:56 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.2\n",
            "11/29/2021 03:38:00 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.2, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:38:00 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:38:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:38:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:38:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:38:00 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:38:00 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:38:00 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:38:00 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:38:00 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.2\n",
            "11/29/2021 03:38:04 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:04 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:38:04 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:38:04 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:38:04 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:11<00:00, 34.50it/s]\n",
            "Eval time: 11.826539278030396\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 46, 7: 21, 8: 0, 9: 6, 10: 88, 11: 80, 12: 166}\n",
            "Expected saving 0.8647875816993464\n",
            "11/29/2021 03:38:16 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:38:16 - INFO - __main__ -     acc = 0.8651960784313726\n",
            "11/29/2021 03:38:16 - INFO - __main__ -     acc_and_f1 = 0.8849377272572807\n",
            "11/29/2021 03:38:16 - INFO - __main__ -     f1 = 0.9046793760831888\n",
            "Result: 0.9046793760831888\n",
            "0.3\n",
            "2021-11-29 03:38:25.157750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:38:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:38:25 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:38:25 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:38:25 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:38:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:38:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:38:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:38:25 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:38:25 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.3\n",
            "11/29/2021 03:38:29 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.3, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:38:29 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:38:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:38:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:38:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:38:29 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:38:29 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:38:29 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:38:29 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:38:29 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.3\n",
            "11/29/2021 03:38:33 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:38:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:38:33 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:38:33 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:38:33 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:38:33 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:10<00:00, 38.15it/s]\n",
            "Eval time: 10.695519208908081\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 0, 5: 21, 6: 98, 7: 38, 8: 1, 9: 7, 10: 76, 11: 59, 12: 108}\n",
            "Expected saving 0.7628676470588235\n",
            "11/29/2021 03:38:44 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:38:44 - INFO - __main__ -     acc = 0.8382352941176471\n",
            "11/29/2021 03:38:44 - INFO - __main__ -     acc_and_f1 = 0.8628036538847621\n",
            "11/29/2021 03:38:44 - INFO - __main__ -     f1 = 0.8873720136518771\n",
            "Result: 0.8873720136518771\n",
            "0.4\n",
            "2021-11-29 03:38:52.771232: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:38:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:38:52 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:38:52 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:38:52 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:38:52 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:38:52 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:38:52 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:38:52 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:38:52 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.4\n",
            "11/29/2021 03:38:57 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.4, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:38:57 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:38:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:38:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:38:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:38:57 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:38:57 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:38:57 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:38:57 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:38:57 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.4\n",
            "11/29/2021 03:39:01 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:01 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:01 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:39:01 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:39:01 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:39:01 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:09<00:00, 43.44it/s]\n",
            "Eval time: 9.393577575683594\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 2, 5: 89, 6: 97, 7: 37, 8: 3, 9: 9, 10: 53, 11: 54, 12: 64}\n",
            "Expected saving 0.6721813725490197\n",
            "11/29/2021 03:39:10 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:39:10 - INFO - __main__ -     acc = 0.8137254901960784\n",
            "11/29/2021 03:39:10 - INFO - __main__ -     acc_and_f1 = 0.8428896811249752\n",
            "11/29/2021 03:39:10 - INFO - __main__ -     f1 = 0.872053872053872\n",
            "Result: 0.872053872053872\n",
            "0.5\n",
            "2021-11-29 03:39:19.238313: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:39:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:39:19 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:39:19 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:39:19 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:39:19 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:39:19 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:39:19 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:39:19 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:39:19 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.5\n",
            "11/29/2021 03:39:23 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.5, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:39:23 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:39:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:39:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:39:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:39:23 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:39:23 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:39:23 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:39:23 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:39:23 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.5\n",
            "11/29/2021 03:39:27 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:28 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:39:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:39:28 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:39:28 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:08<00:00, 48.14it/s]\n",
            "Eval time: 8.476473569869995\n",
            "Exit layer counter {1: 0, 2: 0, 3: 0, 4: 71, 5: 101, 6: 79, 7: 24, 8: 1, 9: 11, 10: 47, 11: 39, 12: 35}\n",
            "Expected saving 0.5835375816993464\n",
            "11/29/2021 03:39:36 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:39:36 - INFO - __main__ -     acc = 0.8161764705882353\n",
            "11/29/2021 03:39:36 - INFO - __main__ -     acc_and_f1 = 0.8463089931194884\n",
            "11/29/2021 03:39:36 - INFO - __main__ -     f1 = 0.8764415156507415\n",
            "Result: 0.8764415156507415\n",
            "0.6\n",
            "2021-11-29 03:39:44.962870: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:39:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:39:45 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:39:45 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:39:45 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:39:45 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:39:45 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:39:45 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:39:45 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:39:45 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.6\n",
            "11/29/2021 03:39:49 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.6, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:39:49 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:39:49 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:39:49 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:39:49 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:39:49 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:39:49 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:39:49 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:39:49 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:39:49 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.6\n",
            "11/29/2021 03:39:53 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:39:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:39:53 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:39:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:39:53 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:39:53 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:06<00:00, 65.55it/s]\n",
            "Eval time: 6.2263195514678955\n",
            "Exit layer counter {1: 0, 2: 88, 3: 7, 4: 150, 5: 44, 6: 38, 7: 15, 8: 2, 9: 10, 10: 31, 11: 13, 12: 10}\n",
            "Expected saving 0.4144199346405229\n",
            "11/29/2021 03:40:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:40:00 - INFO - __main__ -     acc = 0.7720588235294118\n",
            "11/29/2021 03:40:00 - INFO - __main__ -     acc_and_f1 = 0.8118667323388686\n",
            "11/29/2021 03:40:00 - INFO - __main__ -     f1 = 0.8516746411483255\n",
            "Result: 0.8516746411483255\n",
            "0.7\n",
            "2021-11-29 03:40:08.396315: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "11/29/2021 03:40:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/29/2021 03:40:08 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:40:08 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:40:08 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:40:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:40:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:40:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:40:08 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:40:08 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.7\n",
            "11/29/2021 03:40:12 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, early_exit_entropy=0.7, eval_after_first_stage=False, eval_all_checkpoints=False, eval_each_highway=False, eval_highway=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./saved_models/bert-base/MRPC/two_stage', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='./saved_models/bert-base/MRPC/two_stage', output_mode='classification', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=8, plot_data_dir='./plotting/', save_steps=50, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "11/29/2021 03:40:12 - INFO - transformers.tokenization_utils -   Model name './saved_models/bert-base/MRPC/two_stage' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming './saved_models/bert-base/MRPC/two_stage' is a path or url to a directory containing tokenizer files.\n",
            "11/29/2021 03:40:12 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/vocab.txt\n",
            "11/29/2021 03:40:12 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/added_tokens.json\n",
            "11/29/2021 03:40:12 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/special_tokens_map.json\n",
            "11/29/2021 03:40:12 - INFO - transformers.tokenization_utils -   loading file ./saved_models/bert-base/MRPC/two_stage/tokenizer_config.json\n",
            "11/29/2021 03:40:12 - INFO - __main__ -   Evaluate the following checkpoints: ['./saved_models/bert-base/MRPC/two_stage']\n",
            "11/29/2021 03:40:12 - INFO - transformers.configuration_utils -   loading configuration file ./saved_models/bert-base/MRPC/two_stage/config.json\n",
            "11/29/2021 03:40:12 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/29/2021 03:40:12 - INFO - transformers.modeling_utils -   loading weights file ./saved_models/bert-base/MRPC/two_stage/pytorch_model.bin\n",
            "0.7\n",
            "11/29/2021 03:40:16 - INFO - __main__ -   Creating features from dataset file at glue_data/MRPC\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   Writing example 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "11/29/2021 03:40:16 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "11/29/2021 03:40:17 - INFO - __main__ -   Saving features into cached file glue_data/MRPC/cached_dev_two_stage_128_mrpc\n",
            "11/29/2021 03:40:17 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "11/29/2021 03:40:17 - INFO - __main__ -     Num examples = 408\n",
            "11/29/2021 03:40:17 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 408/408 [00:02<00:00, 165.19it/s]\n",
            "Eval time: 2.470792055130005\n",
            "Exit layer counter {1: 408, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}\n",
            "Expected saving 0.08333333333333333\n",
            "11/29/2021 03:40:19 - INFO - __main__ -   ***** Eval results  *****\n",
            "11/29/2021 03:40:19 - INFO - __main__ -     acc = 0.6838235294117647\n",
            "11/29/2021 03:40:19 - INFO - __main__ -     acc_and_f1 = 0.7480253018237863\n",
            "11/29/2021 03:40:19 - INFO - __main__ -     f1 = 0.8122270742358079\n",
            "Result: 0.8122270742358079\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/eval_entropy.sh {PATH_TO_DATA} {MODEL_TYPE} {MODEL_SIZE} {DATASET}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxuy21xladUh",
        "outputId": "e054e910-99fd-47a4-f2bb-8e21dfc86e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plotting/saved_models/bert-base/MRPC/two_stage/each_layer.npy\n",
            "[0.81222707 0.81222707 0.81222707 0.81885125 0.83950617 0.82847896\n",
            " 0.8381877  0.83788122 0.84006462 0.85667752 0.88107203 0.90086957]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.0.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            " 13.458903312683105 1.0 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.001.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            " 13.309908628463745 1.0 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.005.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            " 13.593218088150024 1.0 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.01.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 408}\n",
            " 13.405313730239868 1.0 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.05.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 41, 12: 367}\n",
            " 13.55452013015747 0.9916258169934641 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.1.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 4, 7: 1, 8: 1, 9: 0, 10: 21, 11: 110, 12: 271}\n",
            " 12.941124200820923 0.9622140522875817 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.15.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 15, 7: 14, 8: 2, 9: 3, 10: 60, 11: 112, 12: 202}\n",
            " 12.6563560962677 0.9164624183006536 0.9008695652173914]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.2.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 46, 7: 21, 8: 0, 9: 6, 10: 88, 11: 80, 12: 166}\n",
            " 11.826539278030396 0.8647875816993464 0.9046793760831888]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.3.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 0, 5: 21, 6: 98, 7: 38, 8: 1, 9: 7, 10: 76, 11: 59, 12: 108}\n",
            " 10.695519208908081 0.7628676470588235 0.8873720136518771]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.4.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 2, 5: 89, 6: 97, 7: 37, 8: 3, 9: 9, 10: 53, 11: 54, 12: 64}\n",
            " 9.393577575683594 0.6721813725490197 0.872053872053872]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.5.npy\n",
            "[{1: 0, 2: 0, 3: 0, 4: 71, 5: 101, 6: 79, 7: 24, 8: 1, 9: 11, 10: 47, 11: 39, 12: 35}\n",
            " 8.476473569869995 0.5835375816993464 0.8764415156507415]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.6.npy\n",
            "[{1: 0, 2: 88, 3: 7, 4: 150, 5: 44, 6: 38, 7: 15, 8: 2, 9: 10, 10: 31, 11: 13, 12: 10}\n",
            " 6.2263195514678955 0.4144199346405229 0.8516746411483255]\n",
            "\n",
            "plotting/saved_models/bert-base/MRPC/two_stage/entropy_0.7.npy\n",
            "[{1: 408, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}\n",
            " 2.470792055130005 0.08333333333333333 0.8122270742358079]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "relative_path = \"plotting/saved_models/\"+MODEL_TYPE+\"-\"+MODEL_SIZE+\"/\"+DATASET+\"/two_stage/\"\n",
        "print(relative_path)\n",
        "\n",
        "for path, lists, frame in os.walk(relative_path):\n",
        "    for value in frame:\n",
        "        print(path+value)\n",
        "        data = np.load(path+\"/\"+value, allow_pickle=True)\n",
        "        print(data)\n",
        "        print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeeBERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
